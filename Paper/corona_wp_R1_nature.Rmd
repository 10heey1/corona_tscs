---
title: "CoronaNet: A Dyadic Dataset of Government Responses to the COVID-19 Pandemic"
header-includes:
   - \usepackage{lineno}
   - \linenumbers
csl: nature.csl
author: 
  - Cindy Cheng: 
      email: cindy.cheng@hfp.tum.de
      institute: tum
      correspondence: true
  - Joan Barceló:
      institute: nyuad
  - Allison Spencer Hartnett:
      institute: yale
  - Robert Kubinec:
      institute: nyuad
  - Luca Messerschmidt:
      institute: tum
institute:
  - tum: Hochschule für Politik at the Technical University of Munich (TUM) and the TUM School of Governance
  - nyuad: New York University Abu Dhabi
  - yale: Yale University
date: "April 24th, 2020"
toc: false
bibliography: BibTexDatabase.bib
#always_allow_html: yes
output: 
  #bookdown::word_document2
  bookdown::pdf_document2:
    keep_tex: true
    #latex_engine: xelatex
    includes:
      in_header:
          preamble.tex
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
urlcolor: blue
mask: yes
abstract: "Governments everywhere have implemented a broad range of policies that have been highly influential in shaping the COVID-19 pandemic. We present an initial public release of a large hand-coded dataset of over 10,000 separate policy announcements made in response to the pandemic across more than 190 countries. The dataset will be updated daily, with a 5-day lag for validity checking. We currently document policies across numerous dimensions, including the type of policy implemented; national vs. sub-national enforcement; the specific group targeted by the policy; and the time frame within which the policy is implemented. We further analyze the dataset using a Bayesian measurement model which shows the quick acceleration of high-cost policies across countries beginning in mid-March and continuing to the present. While some relatively low-cost policies like task forces and health monitoring began early, countries generally adopted harsher measures within a narrow time window, suggesting strong policy diffusion effects.^[We thank the very large number of research assistants who coded this data. Their names and affiliations are listed in the appendix. We also thank the Chair of International Relations at the Hochschule für Politik at the Technical University of Munich (TUM) for their support of this project and the TUM School of Management for their help in providing access to Qualtrics. For the most current, up to date version of the dataset, please visit http://coronanet-project.org and also our Github page at https://github.com/saudiwin/corona_tscs. Interested readers may also find our code for collecting the data and maintaining the database at the aforementioned Github page. For more information on the exact variables collected, please see our publicly available  [codebook here](https://docs.google.com/document/d/1zvNMpwj0onFvUZ_gLl4RRjqS-clbHv3TIX6EOHofsME/edit?usp=sharing).]"


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE,tinytex.verbose = TRUE)

# to compile with pandoc
# do brew install pandoc pandoc-citeproc
# then add this line to a .Renviron file in the project folder:
# PATH="/usr/local/Cellar/pandoc/2.9.2.1/bin/:${PATH}"

require(dplyr)
require(tidyr)
require(ggplot2)
require(lubridate)
require(stringr)
require(kableExtra)
require(rstan)
require(ggrepel)
library(readr)
library(knitr)
library(readxl)
require(gghighlight)
require(patchwork)


library(devtools)
library(igraph)
library(remotes)
#remotes::install_github("wjrl/RBioFabric")
library(RBioFabric)

# let's load some data!
# run RCode/cleanData/cleanQualtrics_short.R

clean_data <- read.csv("../data/CoronaNet/coronanet_release.csv", stringsAsFactors = FALSE) %>% 
   mutate(date_announced=as.Date(date_announced, '%Y-%m-%d'),
          date_start=as.Date(date_start, '%Y-%m-%d')) %>% 
  # filter(date_announced<(today()-days(5)),!is.na(country),is.na(init_other),is.na(target_other) | target_other=="")
  filter(!is.na(country),is.na(target_other)| target_other=="")
 
# run /RCode/slack/slackAnalytics.R"

slack <- readRDS("../data/slack/corona_govt_response_slack_latest_clean.rds")

# run /RCode/country_regions.R
regions_df = read.csv( "../data/regions/country_regional_groups_concordance.csv", stringsAsFactors = FALSE)
countries = read.csv('../data/regions/all_countries.csv', stringsAsFactors = FALSE)
```


\newpage
Governments all around the world have implemented an astonishing number and variety of policies in reaction to the COVID-19 pandemic in a very short time frame. However, policy makers and researchers have to date lacked access to the quality, up-to-date data they need for conducting rigorous analyses of whether, how, and to what degree these fast changing policies have worked in brunting the health, political and economic effects of the pandemic. To address this concern, in this paper, we present the CoronaNet COVID-19 Government Response Database which provides fine-grained, dyadic data on policy actions taken by governments across the world since the Chinese government reported the COVID-19 outbreak on December 31, 2019. At the time of writing, the dataset covers the policy actions of `r length(unique(clean_data$country))` countries^[Note, we will include additional countries in future versions of the dataset.] up until `r max(clean_data$date_announced,na.rm=T)`, for a total of `r length(unique(clean_data$record_id))` events. 

With the help of a team of over 220 research assistants in 18 time zones, we are releasing the data on a daily basis. We are implementing a five-day lag between data collection and release to evaluate and validate ongoing coding efforts for random samples of the data to ensure the best possible quality given the considerable time constraints.  More specifically, the CoronaNet database collects daily data on government policy actions taken against COVID-19 across the following dimensions:

+ The type of government policy implemented (e.g. quarantine, closure of schools  [16 total])
+ The level of government initiating the action (e.g. national, provincial)
+ The geographical target of the policy action, if applicable (e.g. national, provincial, municipal)
+ The human or material target of the policy action, if applicable (e.g. travelers, masks)
+ The directionality of the policy action, if applicable (e.g. inbound, outbound, both)
+ The mechanism of travel that the policy action targets, if applicable (e.g. flights, trains)
+ The compliance with the policy action (e.g. mandatory, voluntary)
+ The enforcer of the policy action (e.g. national government, military)
+ The timing of the policy action (e.g. date announced, date implemented)

<!-- The rapid and devastating spread of the SARS-CoV-2 virus has put in stark relief the previously invisible connections among different countries and people. Our dataset illuminates a countervailing kind of network --- it documents not only what actions governments have taken against COVID-19, but how these actions have targeted other geographical regions and the people and resources within them over time. The data, which is publicly available, will allow us to understand among other things, how the effectiveness of different government policies may vary over time or depending on policy actions taken by other governments.  -->

We believe that this data will not only help policy makers and researchers better understand which policies are more effective in addressing the spread and health outcomes of COVID-19 [@flaxman2020], it will also permit crucial inference on the effects COVID-19 has had on societies and economies. Indeed, anecdotal evidence suggests that the pandemic has already had substantial consequences for the nature of political institutions  [@przeworski1999democracy; @gailmard2019preventing], the stability of financial markets [@kindleberger2011manias] and the way of life of billions of people [@tierney2007margins]. Data on government reactions to the COVID-19 pandemic can help provide systematic evidence of these effects. Moreover, it can further help us better understand the determinants of these influential policies at both the structural  [@svolik2012politics; @kitschelt2007patrons] and interpersonal levels [@boin2016politics].

Meanwhile, given the exogenous timing of the initial outbreak in Wuhan, China, government policies made in reaction to the COVID-19 pandemic constitute the single largest natural experiment in recent memory, allowing researchers to improve causal inference in any number of fields. Indeed,  government reactions to the COVID-19 epidemic will have long-lasting implications on a wide-range of social phenomena, from the evolution of political institutions [@pierson2000increasing] to the progression of economic development [@nunn2009importance; @kilian2009not; @noy2009macroeconomic] to say nothing of its potential ramifications for environmental outcomes [@dasgupta2002confronting; @folke2006resilience], mental health [@galea2003trends; @gifford2014environmental], or disaster preparedness [@blaikie2014risk].  While scholars have always sought to understand how large-scale historical events have shaped contemporary phenomena, modern technological tools allow us to document such events more quickly and more precisely than ever before.  

In what follows, we provide a description of the data, as well as an application of the data in which we model policy activity of countries over time. Using a Bayesian dynamic item-response theory model, we produce a statistically valid index that summarizes countries in terms of their response to the pandemic, and further shows how quickly policy responses have changed over time. We document clear evidence of rapid policy diffusion of harsh measures opposing the virus, indicating some of the most extensive evidence of this type of diffusion ever documented. In the methodology section, we provide a thorough discussion of the procedures used to collate the data and to manage the more than 220 research assistants coding this data around the world in real time.

# Results {-}

In this section, we first present some descriptive statistics which illustrate how government policy toward COVID-19 has varied across key variables. We then briefly present our new index for tracking how active governments have been with regards to announcing policies targeting COVID-19 across countries and over time.

## Descriptive Statistics {-}

Here we present some descriptive statistics for key variables available in the data. Table \@ref(tab:desctab) shows the number of records for each policy type, the number of unique countries for each policy type as well as how many countries are targeted in total by each policy type. We note that these are cumulative totals for these different categories in the data, except for the number of targeted countries, which is an average number. Table \@ref(tab:desctab) also provides information on the degree to which a given policy must be complied with.

According to our data, the most common government policy implemented in reaction to COVID-19 is external border restrictions, i.e. policies that seek to limit access to ports of entry or exit across different governmental jurisdictions. We find that `r clean_data %>% filter(type == 'External Border Restrictions') %>% select(country) %>% summarize(length(unique(country)))` countries have made `r clean_data %>% filter(type == 'External Border Restrictions') %>% select(type) %>% summarize(n())` policy announcements about such restrictions since December 31, 2019. Meanwhile, the second policy that most countries, by our count `r clean_data %>% filter(type == 'Closure of Schools') %>% select(country) %>% summarize(length(unique(country)))`, have implemented is 'Closure of Schools', of which we document `r clean_data %>% filter(type == 'Closure of Schools') %>% select(type) %>% summarize(n())` such policies. Governments have implemented 'Restriction of Non-Essential Businesses' policies with the second highest frequency; we document that `r clean_data %>% filter(type == 'Restriction of Non-Essential Businesses')  %>% select(country) %>% summarize(length(unique(country)))` countries  have implemented `r clean_data %>% filter(type == 'Restriction of Non-Essential Businesses')  %>% select(type) %>% summarize(n())` such policies. However, we note that a strict comparison of policy types by this metric is not perfect, given that, for example, there may be a need for more individualized policies regarding external border restrictions (given the number of countries which a government can restrict travel access to) as opposed to closing schools. In the next subsection, we provide a more rigorous method of comparing policies while taking their depth into account. 

Meanwhile, our dataset also shows that virtually all countries in the world are a target of an external border restriction, quarantine measure, or health monitoring measure from another country. Moreover, a high percentage of policies documented in our dataset have mandatory enforcement. 



```{r desctab}

clean_data %>% 
  group_by(type) %>% 
  dplyr:::summarize(`Total Number of Policies`=n(),
            `Number of Countries`=length(unique(country)),
              `Average Number of Targeted Countries`=ifelse(any(grepl('All countries', target_country)),
                                                  c(length(unique(unlist(str_split(target_country, ',')))) -1, 202*length(which(grepl('All countries', target_country)))),
                                                  length(unique(unlist(str_split(target_country, ',')))) ) %>% mean,
                `% With Mandatory Enforcement`=round(mean(grepl(x=compliance,pattern="Mandatory")*100,na.rm=T),0)) %>% 
mutate( type=recode(type,
          `Other Policy Not Listed Above`="Other",
          `New Task Force or Bureau`="New Task Force")) %>% 
  select(Type="type",everything()) %>% 
  filter(!is.na(Type)) %>% 
 arrange(desc(`Total Number of Policies`))  %>% 

  ungroup() %>%
  knitr::kable("latex",booktabs=T,
               caption="Descriptive Information about the CoronaNet Government Response Dataset") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
  column_spec(1,width="4cm") %>% 
  column_spec(2:5,width="2.5cm")

```

In addition, we can look at the cumulative incidence of different types of policies in our data over time, as we show in Figure \@ref(fig:overtime). The figure shows that relatively easy to implement policies like external border restrictions, the forming of task forces, public awareness campaigns, and efforts to increase health resources came relatively earl in the course of the pandemic. More restrictive policies like curfews, closures of schools, restrictions of non-essential businesses and restrictions of mass gatherings arrived later. 

```{r overtime,fig.cap="Cumulative Incidence of Policy Event Types Over Time"}
 
clean_data %>% 
  filter(!is.na(type)) %>% 
  group_by(type,date_announced) %>% 
  summarize(Policies=length(unique(record_id))) %>% 
  arrange(type,date_announced) %>% 
  mutate(Policies=cumsum(Policies)) %>% 
  ungroup() %>% 
        mutate( type=recode(type,
                     `Public Awareness Campaigns`="Public\nAwareness\nCampaigns",
                     `External Border Restrictions`="External\nBorder\nRestrictions",
                     `Other Policy Not Listed Above`="Other",
                     `Restriction of Non-Essential Businesses`="Restriction of\nNon-Essential\nBusinesses",
                     `Restrictions of Mass Gatherings`="Restrictions of\nMass Gatherings",
                     `Restriction of Non-Essential Government Services`="Restriction of\nNon-Essential\nGovernment Services",
                     `Declaration of Emergency`="Declaration of\nEmergency",
                     `Internal Border Restrictions`="Internal\nBorder Restrictions",
                     `External Border Restrictions`="External\nBorder Restrictions",
                     `Public Awareness Campaigns`="Public\nAwareness Campaigns",
                     `New Task Force or Bureau`="New Task Force")) %>% 
  ggplot(aes(y=Policies,x=date_announced)) +
  geom_area() +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        strip.background = element_blank()) +
  xlab("") +
  facet_wrap(~type)

```


We can also explore the extent to which other countries are affected by policies that can have a geographic target outside the policy initiator (e.g. 'external border restrictions', 'quarantine') across time. For example, in Figure \@ref(fig:biofe), we map a network of bans on inbound flights to European countries initiated by European countries^[In this paper, the following countries are defined as being in Europe: Albania, Andorra, Armenia, Austria, Belarus, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Ireland, Italy, Kosovo, Latvia, Liechtenstein, Lithuania, Luxembourg, Macedonia, Malta, Moldova, Monaco, Montenegro, Netherlands, Norway, Poland, Portugal, Romania, San Marino, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Ukraine, United Kingdom, and the Vatican.] as of March 15, 2020. In the plot, each horizontal line represents a potential geographical target of a flight ban. The vertical lines denote whether there was such a flight ban and the arrow of the vertical line indicates the direction in which the ban is applied.^[See @longabaugh2012 for more information on how to interpret this plot.] The figure shows that by March 15, 2020, the governments of Poland and San Marino had banned all flights into Poland and San Marino respectively while the government of Italy banned incoming flights from China, Hong Kong, Macau and Taiwan. Additionally, the governments of Greece and Romania both banned flights from Italy while the government of Albania banned incoming flights from Greece. According to our data, up until this point in time, no other European governments at the national level had banned inbound flights from other countries.^[However, at the provincial level, our dataset documents that the government of the autonomous region of Madeira, Portugal had banned flights from Denmark, Finland, France, Germany, Spain, and Switzerland while the government of Sardinia, Italy closed all airports by March 15, 2020.]

```{r, results='hide'}

# add region variable
clean_data$region = regions_df$regions[match(clean_data$country, regions_df$country)] 

# create numeric ids per country; needed to create igraph 
country_id = factor(unique(countries$Country))
levels(country_id ) = 1:length(country_id )
country_id = as.numeric(as.character(country_id))
cid = data.frame(country_name = countries$Country, country_id)
clean_data$country_id = cid$country_id[match(clean_data$country, cid$country_name)]
clean_data$target_country_id = cid$country_id[match(clean_data$target_country, cid$country_name)]

# user function for plotting biofabric plots by region, policy type, date, travel mechanism and travel direction
'%!in%' <- function(x,y)!('%in%'(x,y))
plotBioGraph = function(regionName, 
                        policyType, 
                        dateStart, 
                        allDum  = FALSE, 
                        travelMech = c('Flights' ,'All kinds of transport (all below except visa restrictions)'),
                        travelDir = c("Inbound", "Inbound/Outbound")){
  
  # subset data
  sub_data = clean_data %>% filter(date_start<= dateStart) 
  sub_data =  sub_data %>% filter(type == policyType ) 
  sub_data = sub_data %>% separate_rows(travel_mechanism, sep = ',')
  sub_data = sub_data %>% filter(travel_mechanism %in% travelMech)  
  sub_data = sub_data %>% filter(target_direction %in% travelDir )  
  
  # mandatory compliance
  sub_data = sub_data %>% separate_rows(compliance, sep = ',' )  
  sub_data = sub_data %>% filter(compliance %!in% "Voluntary/Recommended but No Penalties")  
  sub_data = sub_data %>% filter(target_country!="Other (please specify below)")
  
  
  # conditional logic for only selecting inbound flight bans external border restrctions
  if(policyType == 'External Border Restrictions'){
    sub_data = sub_data %>% filter(type_sub_cat %!in% c("Health Screenings (e.g. temperature checks)", "Health Certificates", "Travel History Form (e.g. documents where traveler has recently been)") )
    sub_data = sub_data %>% filter(target_who_what %in% c("All (Travelers + Residents)", "All Travelers (Citizen Travelers + Foreign Travelers )") )
  }
  
  ## Note, the following entry is not 'wrong', but is not a sweeping travel ban
  # Macau, not travel ban but health declaration doesn't fit into existing categories
  sub_data = sub_data %>% filter(policy_id %!in% c(4638284 ))
  
  # Italian ban only for Sardenia
  sub_data = sub_data %>% filter(policy_id %!in% c(4539256))
  
  
  # Portugal ban only for Maideria
   sub_data = sub_data %>% filter(policy_id %!in% c(6369772))
  ## FIX THE BELOW LATER IN QUALTRICS
  
  # this is not a travel ban for people entering Italy, but for leaving Italy
  sub_data = sub_data %>% filter(policy_id %!in% c(3071523 ))
  
  # this is not a travel ban for people entering Italy,but a health declaration form
  sub_data = sub_data %>% filter(policy_id %!in% c(1727181))
  
  # Micronesia, not a ban on all countries, but unsepcificed countries that have coronacases
  sub_data = sub_data %>% filter(policy_id %!in% c(8819062,3625348, 4465391 ))
  
  # Colombia: this is quarantine upon arrival, not external border restriction
  sub_data = sub_data %>% filter(policy_id %!in% c(1293754 ))
  
  # Bahrain: these are recommendations, not mandatory bans
  sub_data = sub_data %>% filter(policy_id %!in% c(5709516, 9327917 ))
  
  # South Sudan, not a ban on all countries, but on unsepcified countries affected by covid
  sub_data = sub_data %>% filter(policy_id %!in% c(5302981))
  
  # Initiating country is Slovenia, not Iran
  sub_data = sub_data %>% filter(policy_id %!in% c(3031369))
  
  # Germany: target country is not Germany but 'All countries'
  sub_data = sub_data %>% filter(policy_id %!in% c(1592443,3788036, 5258786 ,1592443,3788036,5258786 ))
  
  #Azerbaijan target who what should be 'other' for bsuiness trips/public employees
  sub_data = sub_data %>% filter(policy_id %!in% c( 2632978))
  
  #Tunisia target who what should be 'other' for students
  sub_data = sub_data %>% filter(policy_id %!in% c( 7066331))
  
  # add data for italy's ban of inbound flights from Taiwan, Hong Kong and Macau
  # currently not in the data but we should add these entries
  # https://www.reuters.com/article/china-health-taiwan-italy/italy-says-taiwan-flight-resumption-request-noted-after-virus-ban-idUSL4N2A52YR
  
  sub_data = sub_data %>% add_row(country = 'Italy', target_country = 'Taiwan', type = 'External Border Restrictions', target_direction = 'Inbound', travel_mechanism = "Flights", date_start = as.Date("2020-02-02"), region = 'Europe')
  sub_data = sub_data %>% add_row(country = 'Italy', target_country = 'Hong Kong', type = 'External Border Restrictions', target_direction = 'Inbound', travel_mechanism = "Flights", date_start = as.Date("2020-02-02"), region = 'Europe')
  sub_data = sub_data %>% add_row(country = 'Italy', target_country = 'Macau', type = 'External Border Restrictions', target_direction = 'Inbound', travel_mechanism = "Flights", date_start = as.Date("2020-02-02"), region = 'Europe')
  
  # add data for greece's ban of inbound/outbound flights from Italy
  # https://www.reuters.com/article/us-health-coronavirus-greece-death/greece-reports-two-more-coronavirus-fatalities-suspends-all-flights-to-italy-idUSKBN2110H1
  sub_data = sub_data %>% add_row(country = 'Greece', target_country = 'Italy', type = 'External Border Restrictions', target_direction = 'Inbound', travel_mechanism = "Flights", date_start = as.Date("2020-03-14"), region = 'Europe')
  
  
  # expand dataset to include disaggregated country data when applicable
  sub_data[which(sub_data$target_country == 'All countries'), 'target_country'] = paste(cid$country_name, collapse = ',')
  sub_data = sub_data %>% separate_rows(target_country, sep = ',')
  
  # remove duplicates
  sub_data = sub_data %>%   
    distinct(country, target_country, .keep_all = TRUE)  
  
  # subset to a region, if applicable
  if(allDum == FALSE){
    sub_data = sub_data %>% filter(region %in% regionName)}

  # extract edges and nodes for network
  nodes = data.frame(unique(c(sub_data$country,sub_data$target_country)))
  edges = data.frame(from = sub_data$country,to = sub_data$target_country)

  # create network from subdata
  sub_data_net = graph_from_data_frame(edges, nodes, directed = TRUE)

  #format .pdf size and save pdf of bifabric
  height <- vcount(sub_data_net)
  width <- ecount(sub_data_net)
  aspect <- height / width;
  plotWidth <- 100.0
  plotHeight <- plotWidth * (aspect * 1.2)

  # make biofabric plot
  bioFabric(passthroughNodeOrder(sub_data_net) )
  
}



```

```{r biofe,fig.cap="Network Map of Bans on Inbound Flights by European Countries as of March 15, 2020"}

plotBioGraph("Europe", "External Border Restrictions", "2020-03-15") 
```




# Government Policy Activity Index {-}

In this section, we briefly present our new index for tracking the relative government activity with regards to policies targeting COVID-19 across countries and over time. The model is a version of item-response theory known as ideal point modeling that incorporates over-time trends [@kubinec2019ideal;@jackman2004;@gelman2005;@quinn2002], permitting inference on how a latent construct, in this case total policy activity, responds to changes in the pandemic. To fit the model, the different policy types shown in Table \@ref(tab:desctab) were coded in terms of ordinal values, with lower values for sub-national targets of policies and higher values for policies applying to the entire country, or in the case of external border restrictions, to one or more external countries. For instance, internal country policies can take on three possible values: no policy, sub-national policy, or policy covering the whole country. Meanwhile external border restrictions can take on four possible values: no policy, policy targeting one other country, policy targeting multiple countries, and policy targeting all countries in the world (i.e., border closure).  

We employed ideal point modeling because it can be given a latent utility interpretation [@jackman2004]. The model assumes that countries are located in a latent space in which the distance between countries and policies represents the relative cost of imposing different policies. As countries become more willing to pay these costs, i.e. their ideal points/policy activity score rises, they then subsequently implement more policies. This interpretation is similar to the traditional item-response theory approach for analyzing test questions in which students who answer more questions on a test are considered to have higher "ability" [@takane1986; @reckase2009]. Following this logic, we are able to estimate latent country scores that represent the readiness of a country to impose a set number of policies. The cost of policies is estimated via discrimination parameters, which indicate how strongly policies discriminate between countries (in other words, are an indication of relative cost).

The country-level policy activity score is further allowed to vary over time in a random-walk process with a country-specific variance parameter to incorporate heteroskedasticity [@quinn2002]. Incorporating over-time trends explicitly is very important for capturing the nuances of policy implementation over time. For example, countries that impose more restrictive policies at an earlier date will be rewarded with higher policy activity scores compared to those who impose such policies at a later date. Imposing a given policy when most countries have already imposed them will result in little if any change in the policy activity score. 

The advantage of employing a statistical model, rather than simply summing across policies, is that the index ends up as a weighted average, where the weights are derived from the probability that a certain policy is enforced. In other words, while many countries set up task forces, relatively few imposed curfews at an early stage. As a result, the model adjusts for these distinctions, producing a score that aggregates across the patterns in the data. 

Furthermore, because the model is stochastic, it is robust to some of the coding errors of the kind that often occur in these types of datasets. As we discuss in our validation section, while we are continuing to validate the data on a daily basis, the massive speed and scope of data collection means that we cannot identify all issues with the data in real time. However, the measurement model employed only requires us to assume that on average the policy codings are correct, not that they are correct for each instance. Coding error, such as incorrectly selecting a policy type, will propagate through the model as higher uncertainty intervals, but will not affect average posterior estimates. As our data quality improves, and we are able to collect more data over time, the model will produce more variegated estimates with smaller uncertainty intervals.

Figure \@ref(fig:plotindex) shows the estimated index scores for the `r length(unique(clean_data$country))` countries in our dataset at present, and suggests strong evidence of policy diffusion effects. While information about COVID-19 existed at least as early as January, we do not see large-scale changes occurring in activity scores until March. Furthermore, the trajectories are highly non-linear, with a large number of countries quickly transitioning from relatively low to relatively high scores. This tandem movement is a strong indication of policy diffusion as countries adopted similar policies across time and space as opposed to a more linear learning process.

Of course, a caveat with the index is that we may be missing some possible policy measures that have occurred due to the difficulty in finding them in published sources. However, there is still clear differentiation within the index in terms of when policies were imposed, with some countries starting to impose policies much earlier than others. Furthermore, there is a clear break about March 1st when countries began to impose more stringent policies across the world.



```{r plotindex,fig.cap="CoronaNet Time-Varying Index of National Policy Activity of Measures Opposing COVID-19 Pandemic"}

# severity_fit <- readRDS("../data/activity_fit_rw.rds")
# 
# all_lev <- as.character(unique(clean_data$init_country))
# 
# get_est <- as.data.frame(severity_fit@stan_samples,"L_tp1") %>%
#   mutate(iter=1:n()) %>%
#   gather(key="parameter",value="estimate",-iter) %>%
#   mutate(date_announced=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?[0-9]?0?")),
#          country=as.numeric(str_extract(parameter,"[1-9][0-9]?[0-9]?0?(?=\\])")),
#          country=factor(country,labels=levels(severity_fit@score_data@score_matrix$person_id)),
#          date_announced=factor(date_announced,labels=as.character(sort(unique(severity_fit@score_data@score_matrix$time_id)))))
# 
# saveRDS(get_est,"../data/get_est.rds")

get_est <- readRDS("../data/get_est.rds")
 
 
get_est_sum <- get_est %>%
            ungroup %>%
  filter(country!="Chad") %>%
            mutate(estimate=plogis(estimate)*100,
                   date_announced=ymd(as.character(date_announced))) %>%
  group_by(country,date_announced) %>%
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>%
  group_by(date_announced) %>%
  mutate(`Country Rank`=rank(med_est))
 

list_countries <- c("United States of America",
                   # "United Kingdom",
                    "China",
                    "Taiwan",
                    "Germany",
                    "Singapore",
                    "Italy",
                    "France",
                    "South Korea",
                    "Gabon",
                   # "Croatia",
                    "United Arab Emirates",
                    "Yemen"#,
                   # "St. Lucia"
                    )

random_country <- group_by(get_est_sum,country) %>% 
  sample_n(1)

# need a way to facet this

# facets <- lapply(c("Jan","Feb","Mar","Apr"), function(m) {
#   mutate(get_est_sum,month_facet=m)
# }) %>% bind_rows

to_plot <- get_est_sum %>%
  ungroup %>%
  mutate(month_var=month(date_announced,label=T)) %>% 
  group_by(month_var,country) %>% 
  mutate(country_spread=unique(med_est[date_announced==max(date_announced)]) - unique(med_est[date_announced==min(date_announced)])) %>% 
  ungroup

calc_ranks <- select(to_plot,country_spread,country,month_var) %>% distinct %>% 
  arrange(month_var,desc(country_spread)) %>% 
  group_by(month_var) %>% mutate(count_rank=rev(rank(country_spread)))
    
  
  
  breakout <- to_plot  %>%
    left_join(calc_ranks,by=c("country","country_spread","month_var")) %>% 
  filter(month_var!="Dec") %>% 
  ungroup %>% 
  ggplot(aes(y=med_est,x=date_announced)) +
  #geom_ribbon(aes(ymin=low_est,ymax=high_est,group=country),alpha=0.2) +
  geom_line(aes(group=country,colour=med_est),alpha=0.7, size = 1) +
  scale_color_distiller(palette="Reds",direction=1) +
  gghighlight(count_rank<5,
              calculate_per_facet = T,
              label_params=list(size=2.5,colour="black"),
              unhighlighted_params = list(size = .5, colour = alpha("grey", 0.3))) +
  # scale_color_manual(name="Country",values=c('#00CC33','#E69F00','#CC0000',"#FF99FF","#99CCFF","#006699","#990099","orangered1","#9933FF","yellow3", "blue1" ))+
  theme_minimal() +
  theme(panel.grid = element_blank(),
        axis.text.x=element_text(angle=45)) +
  xlab("") +
  ylab("") +
  facet_wrap(~month_var,scales = "free_x")
  
overall <- to_plot  %>%
  group_by(date_announced) %>% 
  mutate(all_rank=rev(rank(med_est))) %>% 
  ungroup %>% 
    mutate(last_rank = ifelse(date_announced==max(date_announced),`Country Rank`,NA),
         lab_country = ifelse(last_rank %in% c(1,10,20,30,50,100,190),as.character(country),
                              NA)) %>% 
  ungroup %>% 
  ggplot(aes(y=med_est,x=date_announced)) +
  #geom_ribbon(aes(ymin=low_est,ymax=high_est,group=country),alpha=0.2) +
  geom_line(aes(group=country,colour=med_est),alpha=0.7, size = 1) +
  scale_color_distiller(palette="Reds",direction=1) +
  geom_label_repel(aes(label=lab_country),size=3) +
  # scale_color_manual(name="Country",values=c('#00CC33','#E69F00','#CC0000',"#FF99FF","#99CCFF","#006699","#990099","orangered1","#9933FF","yellow3", "blue1" ))+
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  # labs(caption="Estimates are derived from Stan, a Markov Chain Monte Carlo sampler.\nThe intervals represent the median and 5% - 95% posterior density region.\nPlot shows one estimate per country per day.") +
  xlab("") +
  ylab("Policy Activity Index Scale (0 to 100)") +
  guides(color="none")

overall + breakout +
  plot_annotation(tag_levels="A",
                  caption=str_wrap("Estimates are derived from Stan, a Markov Chain Monte Carlo sampler. Median posterior estimates are shown. Plot A shows the full distribution of countries, while plot B shows each month separately with the top 4 countries for that month in terms of increases in activity scores from start of the month to the end of the month.",width=115))

ggsave("index.png")




```


Table \@ref(tab:rankcount) shows the discrimination parameters from the underlying Bayesian model for each policy type. These parameters suggest which policies governments find relatively difficult or costly to implement, and for that reason tend to separate more active from less active states in terms of response to COVID-19. Two of these policies (Closure of Restaurants and Quarantine at Home) were given fixed values in order to identify the direction and rotation of the latent scale, and so their discrimination parameters are not informative. However, the rest of the parameters were allowed to float, which provides inference as to which policies appear to be the most difficult/costly to implement.

We note that these are average values for the sample. Imposing these policies may be less costly for certain countries or for countries that share certain characteristics, such as smaller numbers of enrolled students or relatively healthy economies. However, it is important to note that we can see these patterns on a world-wide scale.


Surprisingly, at the top of the index we see school closure as the most difficult/costly policy to implement. Closure of pre-schools, though, as opposed to other school types, appears to be relatively less costly for states to undertake, perhaps because pre-schools do not operate on a full-time basis. Generally speaking, the next most difficult policies are various business closure policies and mandatory social distancing policies. Internal border restrictions are considered more difficult to implement than external border restrictions, while relatively straightforward policies like public awareness campaigns, health monitoring and opening new task forces or bureaus are near the bottom of the index. 

Given this distribution of discrimination parameters, we believe the index is a valid representation of the underlying process by which governments progressively impose more difficult policies. As states relax policies, we will further gain information about which policies appear to be more costly as we will be able to factor in the duration for which these policies were implemented. Consistent with our findings, we observe that the announced relaxation policies happening at the time of writing in European countries primarily center on businesses and school openings, suggesting that these policies are uniquely costly to keep in place compared to travel restrictions.^[See Doherty, Ben. "The exit strategy: how countries around the world are preparing for life after Covid-19." *The Guardian* 18 April 2020, https://www.theguardian.com/world/2020/apr/19/the-exit-strategy-how-countries-around-the-world-are-preparing-for-life-after-covid-19]



```{r rankcount}

# out_items <- summary(severity_fit,pars="items") %>%
#   filter(`Item Type`=="Non-Inflated Discrimination")
# 
# saveRDS(out_items,'../data/out_items.rds')

out_items <- readRDS("../data/out_items.rds")

out_items %>% 
  mutate(`Item Name`=recode(`Item Name`, `Closure of Schools`="All Schools",
                `Closure of Schools_type_highered`="Higher Ed Closure",
                `Closure of Schools_type_preschool`="Pre-school Closure",
                `Closure of Schools_type_primaryschool`="Primary School Closure",
                `Closure of Schools_type_secondschool`="High School Closure",
                `External Border Restrictions_type_screenings`="Border Health Screenings",
                `Health Resources`="General Health Resources",
                `Health Resources_type_health_masks`="Masks Policies",
                `Health Resources_type_health_other`="Other Health Resources",
                `Health Resources_type_health_staff`="Health Staff",
                `Health Resources_type_sanitizer`="Sanitizer Policies",
                `Health Resources_type_temporary`="Temporary Medical Units",
                `Health Resources_type_testing`="Test Production",
                `Quarantine/Lockdown`="Other Quarantines",
                `Quarantine/Lockdown_type_govt_quar`="Quarantine in Govt. Facility",
                `Quarantine/Lockdown_type_quar_restrict`="Limited Quarantine",
                `Quarantine/Lockdown_type_screenings`="Quarantine Screenings",
                `Quarantine/Lockdown_type_self_quarantine`="Quarantine At Home",
                `Restriction of Non-Essential Businesses`="General Business Restrictions",
                `Restriction of Non-Essential Businesses_type_bars`="Closure of Restaurants",
                `Restriction of Non-Essential Businesses_type_grooming`="Closure of Personal Grooming",
                `Restriction of Non-Essential Businesses_type_retail`="Closure of Retail Stores",
                `Restriction of Non-Essential Businesses_type_shopping`="Closure of Shopping Malls")) %>% 
  select(Policy="Item Name",`5% Low Estimate`="Low Posterior Interval",
         `Median Estimate`="Posterior Median",
         `95% High Estimate`="High Posterior Interval") %>%
  mutate(`Median Estimate`=ifelse(Policy=="Closure of Restaurants",
                                  `Median Estimate`*-1,
                                  `Median Estimate`),
         `5% Low Estimate`=ifelse(Policy=="Closure of Restaurants",
                                  `5% Low Estimate`*-1,
                                  `5% Low Estimate`),
         `95% High Estimate`=ifelse(Policy=="Closure of Restaurants",
                                  `95% High Estimate`*-1,
                                  `95% High Estimate`)) %>% 
  arrange(desc(`Median Estimate`)) %>%
  mutate_at(c("5% Low Estimate","Median Estimate","95% High Estimate"),~round(.,1)) %>%
    knitr::kable("latex",booktabs=T,longtable=T,
               caption="Discrimination of Item Parameters (Policies) in Policy Activity Index") %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  column_spec(1,width="4cm") %>%
  column_spec(2:5,width="2.5cm")
```



# Methods {-}

In this section, we first describe the variables that our dataset is able to provide as well as how they are organized. We then provide detail on the methodology we employed to collect the data. 

## Time-Varying Item Response Model

Our time-varying item response model follows the specification in @kubinec2019. We review that notation here to show how it relates to classical item-response theory as well as the ideal point modeling literature. 

The likelihood function for the model is as follows for a set of countries $i \in I$, items $j \in J$, time points $t \in T$ and ordinal categories $k \in K$:

\begin{align}
	L(Y_{ijtk}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T}
	\begin{cases} 
	1 -  \zeta(\gamma_j \alpha_{it} - \beta_j - c_1) & \text{if } K = 0 \\
	\zeta(\gamma_j \alpha_i - \beta_j - c_{k-1}) - \zeta(\gamma_j \alpha_{it} - \beta_j - c_{k})       & \text{if } 0 < k < K, \text{ and} \\
	\zeta(\gamma_j \alpha_{it} - \beta_j - c_{k-1}) - 0 & \text{if } k=K
	\end{cases}
(\#eq:basic)
\end{align}

In this equation, the time-varying country parameters $\alpha_{it}$, also called person abilities or ideal points, are our estimate of policy activity scores. They are estimated jointly with the item (polity type) discrimination parameters $\gamma_j$ and item difficulty (intercept) parameters $\beta_j$. To address the ordinal nature of the outcome $Y_{ijtk}$, ordinal cutpoints $c_{k}$ are used to model the varying levels of enforcement and geographical targets in the data. The logit function, represented by $\zeta(\cdot)$, maps the latent scale to probability that a given ordinal outcome is chosen. Because we have two separate type of ordered measures (domestic versus international policies) with either four or five ordered categories, we estimate the model jointly as two ordered logit specifications. 

The likelihood in \@ref(eq:basic) is not fully identified due to possibly scaling issues with the latent variable $\alpha_{it}$ (i.e., it has no natural units) and due to potential sign reflection (also called multi-modality) where $L(Y_{ijtk})$ could be unchanged even if $\alpha_{it}$ is multiplied by -1. These identification issues are well-known in the literature [@gelman2005], and we resolve them with standard practices. First, we assign a reasonably informative prior distribution on the $t=1$ ideal points:

\begin{equation}
\alpha_{it=1} \sim N(0,1)
(\#eq:id1)
\end{equation}

We also fix the discrimination parameters $\gamma_j$ for two items, quarantines and restriction of restaurants and bars, to opposite ends of the latent scale (+1 and -1). Because both of these variables load on the same side of the scale (i.e. both indicate more policy activity), we reverse the order of the categories for restriction of restaurants and bars. We note that these types of restrictions are not commonly used in traditional IRT, where instead a sign restriction is imposed on all discrimination parameters. We employ the more flexible ideal point specification, which also allows us to test the assumption that all the discrimination parameters load on the same sign (as Table \@ref(tab:rankcount) shows, this is true for all of the parameters). The rest of the parameters are given weakly informative prior distributions:^[A prior is put over the difference of cutpoints, rather than the cutpoints themselves, to reflect the fact that only the differences between cutpoints have any natural scale.]

\begin{align}
\gamma_j &\sim N(0,5)\\
\beta_j &\sim N(0,2)\\
c_k - c_{k-1} &\sim N(0,5)
(\#eq:id2)
\end{align}

Finally, to model the policy scores $\alpha_{it}$ as a random walk, we assign a prior to that is equal to the prior period policy score plus Normally-distributed noise:

\begin{equation}
\alpha_{it} \sim N(\alpha_{it-1},\sigma_i)
(\#eq:rwc)
\end{equation}

The over-time dimension induces a new source of identifiability issues, which we resolve by fixing the variance $\sigma_i$ of one of the countries (the United States) to 0.1 so that the over-time variance is relative to this constant. This constraint has a similar identification effect to the informative prior on the first period policy activity scores in \@ref(eq:id1).


## Model Convergence {-}

For estimation, we sample from four Markov Chain Monte Carlo (MCMC) chains with over-dispersed starting values using Stan, a Hamiltonian Markov Chain Monte Carlo (HMC) sampler [@carpenter2017]. We run the sampler for 600 iterations, 300 of which are discarded as warmup. While this number of iterations is far less than other MCMC samplers, HMC is a far more efficient at exploring the posterior density and we are able to achieve convergence using this number of iterations. 

We assess convergence using split-Rhat as we fit independent chains. All Rhat values were [FILL IN]. We also assess convergence using trace plots, one of which is shown below for the time-varying country policy activity scores for the United States. Strong mixing between chains can be observed in the plot. Finally, we report no divergent transitions or iterations where the sampler reached its maximum tree depth, which are both signs of poor mixing in the chains. For these reasons, we are confident than the sampler reached a stationary distribution and was able to adequately explore the high-density regions of the joint posterior.





## Data Schema {-}

Each policy records at the minimum, the following monadic information: the policy type, the name of the country from which a policy originates,^[If the policy originates from a province or state, that information is also documented. Future versions of the dataset will also include information on whether a policy was initiated from a city or municipality or another level of government.] the degree to which a policy must be complied with, the entity enforcing the policy, and the date a policy is announced, implemented and ends.^[Note that sometimes policies are announced without a pre-determined end date. In those cases, this field is left blank.]  When a policy is dyadic in nature, the database further documents information about the geographic target of the policy, the human or material target of a policy, the directional flow of the policy, and the mechanism of travel. Where applicable, all of the information documented above is also provided qualitatively via a textual policy description. Additional meta-data that is available for all policies include when the record entered into the database and a link for the information source for the policy. See the appendix for a list of currently available fields in the data, along with a list of external data variables such as country-level covariates that are added in to daily releases, including COVID-19 tests and cases. 
 <!---We will also be including a variable which documents which institution is responsible for enforcing a certain policy (e.g. national government, military).
 --->

```{r vardesc, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}

 # `Record ID` = c('record_id', 'A unique identifier for each policy record', 'This variable takes on a numeric value. A unique record ID is given for the following unit of analysis: country-type-date_announced')
 # 
 # `Policy ID` = c('policy_id', 'A unique identifier for each policy record as it is updated over time', 'This variable takes on a numeric value. A unique policy ID encompasses different record IDs that change over time with regards to either the strength or time duration of the underlying policy. ')
 # 
 # `Event Description` = c('event_description', "This variable provides a qualtiative summary of the documented policy", "All qualitative descriptions document at a minimum the following information: the policy type (type), the name of the country from which a policy originates (country); the date a policy is implemented (date_start) and if applicable:  the country or region that a policy is targeted towards (target_country), the type of people or resources a policy is targeted towards (target_who_what), when a policy is slated to end (date_end)")
 # 
 # Type = c("type", 'This variable documents the policy action initiated. It can take on only one of the following values:',
 #          paste(c('Declaraction of Emergency',
 #                  'Quarantine',
 #                  'External Border Restrictions',
 #                  'Internal Border Restrictions',
 #                  'Restrictions of Mass Gatherings',
 #                  'Social Distancing',
 #                  'Curfew',
 #                  'Closure of Schools',
 #                  'Restriction of Non-Essential Government Services',
 #                  'Restriction of Non-Essential Businesses',
 #                  'Health Monitoring',
 #                  'Health Testing',
 #                  'Health Resource',
 #                  'Public Awareness Campaigns',
 #                  'New Task Force or Bureau', 
 #                  'Other'), collapse = ", "))
 # 
 # 'init_country_level' = c("init_country_level", 'This variable documents the level of government from which a policy initiates, where applicable, and can take on only one of the following values  (The exact geographical targets are also documented in other variables* in the dataset):',
 #             paste(c('National',
 #                     'Provincial/State'), collapse = ', '))
 # 
 # `Target Geographic Level` = c("target_geog_level", 
 #                               'This variable documents the geographical target of the policy. It can take on of the following variables (The exact geographical targets are also documented in other variables* in the dataset):',
 #                               paste(c('All countries',
 #                                       'One or more countries and one or more regional groupings',
 #                                       'One or more countries, but not all countries',
 #                                       'One or more regional groupings',
 #                                       'A geographical or administrative unit within a country'), collapse = ', '))
 # 
 # `Target Who What` = c("target_who_what",  'This variable documents the human or material targets of a policy, where applicable, and can take on only one of the following values:',
 #                       paste(c('All (Travelers + Residents)',
 #                               'All Travelers (Citizen Travelers + Foreign Travelers)',
 #                               'Citizen Travelers',
 #                               'Foreign Travelers',
 #                               'All Residents (Citizen Residents + Foreign Residents)',
 #                               'Citizen Residents',
 #                               'Foreign Residents',
 #                               'All Foreign Nationals',
 #                               'All Citizens',
 #                               'Health Staff',
 #                               'Health-related Supplies' ), collapse = ', '))
 # 
 # `Target Direction` = c('target_direction', 'This variable documents the direction of travel a policy targets, where applicable, and can take on only one of the following values:',
 #                        paste(c('Inbound', 
 #                                'Outbound',
 #                                'Inbound/Outbound'), collapse = ', '))
 # 
 # `Travel Mechanism` = c('travel_mechanism', 'This variable documents the mode of travel a policy targets, where applicable, and can take on one or more of the following values:',
 #                        paste(c('All Mechansims (except visa restrictions)',
 #                                'Flights',
 #                                'Land Border',
 #                                'Trains',
 #                                'Buses',
 #                                'Seaports',
 #                                'Ferries',
 #                                'Cruises',
 #                                'Visas'), collapse = ', '))
 # 
 # Compliance = c('compliance',
 #                'This variable documents degree to which a policy must be complied with and can take on one or more of the following values:',
 #                paste(c('Mandatory with Legal Penalties',
 #                        'Mandatory with Fines',
 #                        'Mandatory with Excpetions',
 #                        'Recommended/Voluntary'), collapse =', '))
 # 
 # 
 # Enforcer = c('enforcer', 'This variable documents the entity enforcing a policy and can take on one or more of the following values:',
 #              paste0(c('National Government',
 #                       'Ministry/Department of Health',
 #                       'Military',
 #                       'Provincial/State Government',
 #                       'Municipal/City Government',
 #                       'Police',
 #                       'Other'), collapse =', '))
 # 
 # 
 # date_announced = c('date_announced', 'This variable documents the date when a policy was announced, it takes on the following format:', 'Month-Day-Year')
 # 
 # date_start = c('date_start', 'This variable documents the date when a policy is implemented, it takes on the following format:', 'Month-Day-Year')
 # 
 # date_end = c('date_end', 'This variable documents the date when a policy is slated to end, where applicable, it takes on the following format:', 'Month-Day-Year')
 # 
 #  
 # varDf = rbind(`Record ID`,
 #               `Policy ID` ,
 #             `Event Description`,
 #             Type ,
 #            init_country_level,
 #             Compliance ,
 #             Enforcer ,
 #             date_announced,
 #             date_start,
 #             date_end,
 #             `Target Geographic Level`,
 #             `Target Who What` ,
 #             `Target Direction`,
 #             `Travel Mechanism`) %>% 
 #             data.frame()
 # 
 # names(varDf) = c('Variable Name', 'Description', "Values") 
 # row.names(varDf) = NULL
 # 
 # varDf%>% 
 #   knitr::kable("latex",booktabs=T, longtable = T,
 #                caption="Description of Variables in CoronaNet Government Reponse Dataset") %>%
 #   kable_styling(latex_options = c("striped", "hold_position", "condensed"))%>% 
 #    kable_styling(fixed_thead = T)%>% 
 #    kable_styling(font_size = 10 )%>% 
 #   column_spec(1,width="3.5cm", bold = TRUE) %>% 
 #   column_spec(2,width="5cm")%>%
 #   column_spec(3,width="8.5cm") %>%
 #   row_spec(0,bold=TRUE) %>% 
 #   pack_rows("Monadic Variables", 4, 10, label_row_css = "background-color: #666; color: #fff;") %>%
 #   pack_rows("Dyadic Variables",11, 14, label_row_css = "background-color: #666; color: #fff;") %>%
 # footnote(symbol = "The variables which document the exact geographical initiators of a policy are as follows:  country (documents the country from which a policy is initiated from), init_prov (documents the province from which a policy is initated from). The variables which document the exact geographical targets in the dataset are as follows: target_region (documents targeted regional grouping, e.g. Schengen region), target_country (documents targeted country), target_province (documents targeted province/state). Other variables in the dataset not listed above include: record_date (when the record entered into our data) and link (a link to at least one source for the policy)",threeparttable = T,fixed_small_size = TRUE,footnote_as_chunk = T) 
```


 There is a unique record ID for each unique policy announcement, which we code at the policy sub category type.^[That is, some policy types are further categorized into sub-categories. E.g. 'Quarantine/Lockdown' can be further classified into one or more of the following sub categories: 'Self-Quarantine', 'Government Quarantine', 'Quarantine outside the home or government facility', 'Quarantine only applies to people of certain ages' and 'Other'.] Of the `r length(unique(clean_data$record_id))` such events in the dataset, we have identified `r length(unique(clean_data$record_id[clean_data$entry_type %in% c("correction","new_entry")]))` unique events. That is, some events in the database are updates or changes to existing policies. We link such events over time using a unique ID, which we term the policy ID as opposed to the record ID. An event counts as an update if it deals with a change in either the:

1. Time duration or^[E.g. A country lengthens its quarantine to 28 days from 14 days.]
2. Strength of an existing policy in terms of either:
   a. the nature of the policy^[E.g. People can no longer leave their houses to go to work whereas before they could]
   b. compliance rules for the policy^[E.g. The quarantine used to be voluntary but now its mandatory] 
   c. who the policy applies towards^[E.g. The quarantine used to apply to people of all ages and now it only applies to the elderly.]

A policy counts as a new entry and not an update if it deals with a change in any other dimension, e.g. policy type, targeted country.

## Data Collection Methodology {-}
As researchers learn more about the various health, economic, and social effects of the COVID-19 pandemic, it is crucial that they have access to data that is reliable, valid, and timely (to the greatest extent possible). We have adopted a data collection methodology that we believe optimizes over all three of these constraints.

To collect the data, we recruited more than 220 research assistants (RAs) from colleges and universities around the world, representing 18 out of the 24 time zones.^[For more information on the individual RAs, please visit http://coronanet-project.org/]  Large social scientific datasets typically rely on experts, coders, or crowd-sourcing to input data. The literature has shown that common coding tasks can be completed via crowd-sourcing [@benoitetal2016; @sumneretal2019], but that there are also limitations to the wisdom of crowds when specific contextual or subject knowledge is required [@marquardtetal2017]. To address these trade offs, we decided to train current students to code our entries, leveraging the benefits of wide-spread recruitment and a diverse pool of country-specific knowledge from across the globe. Data collection started on March 28, 2020 and has proceeded rapidly, reaching `r length(unique(clean_data$record_id))` records as of the date of this article. Each RA is responsible for tracking government policy actions for at least one country. RAs were allocated depending on their background, language skills and expressed interest in certain countries [@Horn2019].^[Note depending on the level of policy coordination at the national level, certain countries were assigned multiple RAs, e.g. the United States, Germany, or France.]

We have also partnered with the machine learning company Jataware to automate the collection of more than 200,000 news articles from around the world related to COVID-19.^[We thank Brandon Rose and Jataware for making the news database available to this project.] Jataware employs a natural language processing (NLP) classifier using Bidirectional Encoder Representations from Transformers (BERT) to detect whether a given article is indicative of a governmental policy intervention related to COVID-19. They then apply a secondary NLP classifier to categorize the type of policy intervention (e.g. "declaration of emergency", "quarantine", "travel restrictions", etc.). Next, Jataware extracts the geospatial and temporal extent of the policy intervention (e.g. “Washington DC” and “March 15, 2020”) whenever possible. The resulting list of news sources is then provided to our RAs for manual coding and further data validation.

In what follows, we describe in greater detail how RAs document the policies that they identify using our data collection software instrument, and our post data-collection validation procedure. Please refer to the appendix for more information on our procedure for on-boarding and training RAs and our system for communicating with and organizing RAs.


### Data Collection Software Instrument {-}

We designed a Qualtrics survey with survey questions about different aspects of a government policy action to streamline the CoronaNet data collection effort. With this tool, RAs can easily and efficiently document different policy actions by answering the relevant questions posed in the survey.^[See @buthe2020 for an example of a similar use of Qualtrics in collecting data.] For example, instead of entering the country that initiated a policy action into a spreadsheet, RAs answer the following question in the survey: "From what country does this policy originate from?" and choose from the available options given in the survey.

By using a survey instrument to collect data, we are able to systematize the collection of very fine-grained data while avoiding coding errors common to tools like shared spreadsheets. The value of this approach of course, depends on the comprehensiveness of the questions posed in the survey, especially in terms of the universe of policy actions that countries have implemented against COVID-19. For example, if the survey only allowed RAs to select 'quarantines' as a government policy, it would not capture any data on 'external border restrictions', which would seriously reduce the value of the resulting data. 

As such, to ensure the comprehensiveness of the data, before designing the survey, we collected in depth, over-time data on policy actions taken by one country, Taiwan, since the beginning of the outbreak as well as cross-national data on travel bans implemented by most countries for a total of 245 events.^[The specific data source we cross referenced for this effort was the March 20, 2020 version of the following New York Times article: Salcedo, Andrea and Gina Cherelus, "Coronavirus Travel Restrictions, Across the Globe" *New York Times*, 20 March 2020, https://www.nytimes.com/article/coronavirus-travel-restrictions.html] We chose to focus on Taiwan on because of its relative success, as of March 28, 2020, in limiting the negative health consequences of COVID-19 within its borders.^[Beech, Hannah. "Tracking the Coronavirus: How Crowded Asian Cities Tackled an Epidemic." *New York Times* 18 March 2020, https://www.nytimes.com/2020/03/17/world/asia/coronavirus-singapore-hong-kong-taiwan.html] As such, it seems likely that other countries may choose to emulate some of the policy measures that Taiwan had implemented, which helps increase the comprehensiveness of the questions we ask in our survey. Meanwhile, by also investigating variation in how different countries around the world have implemented travel restrictions, we have also helped ensure that our survey is able to comprehensively document variation in how an important and commonly used policy tool is applied, e.g. restrictions of different methods of travel (e.g. flights, cruises), restrictions across borders and within borders, restrictions targeted toward people of different status (e.g. citizens, travelers).

<!-- As a last step, the team also consulted the ACAPS COVID-19: Government Measures Dataset^[https://data.humdata.org/dataset/acaps-covid19-government-measures-dataset] to validate the comprehensiveness of the policy measures considered in the survey instrument.  -->

<!-- To further address concerns about the comprehensiveness of our data, the survey instrument also allows for a degree of flexibility in learning about new policies that we might not have considered when designing the survey with the use of text entry fields that allows RAs to choose 'Other' categories that. To date, X% of the the data has been coded as 'Other' suggesting that [....]. Please see the descriptive statistics in the Data section for more information.  -->

There are many additional benefits of using a survey instrument for data collection, especially in terms of ensuring the reliability and validity of the resulting the data:

1. *Preventing unforced measurement error.* RAs are prevented from entering data into incorrect fields or unknowingly overwriting existing data---as would be possible with manual data entry into a spreadsheet---because RAs can only document one policy action at a time in a given iteration of a survey and do not have access to the full spreadsheet when they are entering in the data. 

2. *Standardizing responses.* We are able to ensure that RAs can only choose among standardized responses to the survey questions, which increases the reliability of the data and also reduces the likelihood of measurement error. For example, when RAs choose different dates that we would like them to document (e.g., the date a policy was announced) they are forced to choose from a calendar embedded into the survey which systematizes the day, month and year format that the date is recorded in. 

3. *Minimizing measurement error.* A survey instrument allows coding different conditional logics for when certain survey questions are posed. This technique obviates the occurrence of logical fallacies in our data. For example, we are able to avoid situations where an RA might accidentally code the United States as having closed all schools in another country.

4. *Reduction of missing data.* We are able to reduce the amount of missing data in the dataset by using the forced response option in Qualtrics. Where there is truly missing data, there is a text entry at the end of the survey where RAs can describe what difficulties they encountered in collecting information for a particular policy event. 

5. *Reliability of the responses.* We increase the reliability of the documentation for each policy by embedding descriptions of different possible responses within the survey. For example, in the survey question where RAs are asked to identify the policy type (`type` variable, see appendix and/or Codebook), the survey question includes pop-up buttons which allow RAs to easily access descriptions and examples of each possible policy type. Such pop-up buttons were also made available for the survey questions which code for the people or materials a policy was targeted at (`target_who_what`) and whether the policy was inbound, outbound or both (`target_direction`). Embedding such information in the dataset both clarifies the distinction between different answer choices and increases the efficiency of the policy documentation process (as RAs are not obliged to refer back and forth from the survey to the codebook). 

6. *Linking observations.* The use of a survey instrument allows us to easily link policy events together over time should there be updates to existing policies. Once coded, each policy is given a unique Record ID, which RAs can easily look up, reference and link to if they need to update a particular policy.



### Post-Data Collection Validation Checks {-}

We further implement the following processes to validate the quality of the dataset:

1. Cleaning. Before validation, we use a team of RAs to check the raw data for logical inconsistencies and typographical errors.

2. Multiple Coding for Validation. Others have shown that the random allocation of tasks and the validation of labels by more than one coder are among the best ways to improve the quality of a dataset [@Sheng2008;@MTurk2011]. We randomly sample 10% of the dataset using the source of the data (e.g. newspaper article, government press release) as our unit of randomization. We use the source as our unit of randomization because one source may detail many different policy types. We then provide this source to a fully independent RA and ask her to code for the government policy contained in the sampled source in a separate, but identical, survey instrument. If the source is in a language the RA cannot read, then a new source is drawn. The RA then codes all policies in the given source. This practice is repeated a third time by a third independent coder. Given the fact that each source in the sample is coded three times, we can assess the reliability of our measures and report the reliability score of each coder.

3. Evaluation and Reconciliation. We then check for discrepancies between the originally coded data and the second and third coding of the data through two primary methods. First, we use majority-voting to establish a consensus for policy labels. Using the majority label as an estimate of the "hidden true label" is a common method to address classification problems [@Raykar2009]. One issue with this approach is that it assumes that all coders are equally competent [@Raykar2010]. This criticism is generally levied at data creation with crowd-sourced laborers. We mitigate this problem by training our RAs in the data collection process and prioritizing RA country-knowledge and language skills, and therefore ensuring a more equal baseline for RA quality. We provide RA ID codes that will allow users to evaluate coder accuracy.

If the majority achieves consensus, then we consider the entry valid. If a discrepancy exists, a fourth RA or PI evaluates between the three entries to determine whether one, some, a combination of all three is most accurate. Reconciled policies are then entered into the dataset as a correction for full transparency. If an RA was found to have made a coding mistake, then we sample six of their previous entries: 3 entries which correspond to the type of mistake made^[e.g. if the RA incorrectly codes an 'External Border Restriction' as a 'Quarantine', we sample 3 entries where the RA has coded a policy as being about a 'Quarantine.'] and randomly sample 3 more entries to ascertain whether the mistake was systematic or not. If systematic errors are found, entries coded by that individual will be entirely recoded by a new RA.

# Conclusion {-}

As policymakers, researchers and the broader public debate and compare how to succeed against the novel threats posed by COVID-19, they need real-time, traceable data on government policies in order to understand which of these policies are effective, and under what conditions.  This requires specific knowledge of the variation in such policies and the extent of their implementation across countries and time. The goal of the dataset and policy action index presented here is to provide this information. 

We have tried to match our data collection efforts to keep up with the exponential speed with which COVID-19 has already upended global public health and the international economy while also maintaining high levels of quality. However, we will inevitably be refining, revising and updating our data to reflect new knowledge and trends as the pandemic unfolds. The data that we present in this first version of the dataset represents only the initial release of the data, and we will continue to validate and release data so long as governments continue to develop policies in response to COVID-19.

In future work, we intend to analyze the policy combinations that are best able to stymie the epidemic so as to contribute to the social science research community and provide urgently needed knowledge for policymakers and the wider global community.

\newpage
## Data Availability {-}

For the most current, up to date version of the dataset, please visit http://coronanet-project.org or our Github page at https://github.com/saudiwin/corona_tscs. 


## Code Availability {-}

Interested readers may also find our code for collecting the data and maintaining the database at our Github page: https://github.com/saudiwin/corona_tscs.  

## Appendix A: Description of Dataset Fields {-}

The format of the data is in country-day-`record_id` format. Some `record_id` values have letters appended to indicate that the general policy category `type` also has a value for `type_sub_cat`, which contains more detail about the policy, such as whether health resources refers to masks, ventilators, or hospitals. Some entries are marked as `new_entry` in the `entry_type` field for when a policy of that type was first implemented in the country. Later updates to those policies are marked as updates in `entry_type`. To see how policies are connected, look at the `policy_id` field for all policies from the first entry through updates for a given country/province/city. If an entry was corrected after initial data collection, it will read corrected in the `entry_type` field (the original incorrect data has already been replaced with the corrected data). 

1. **`coronanet_release.csv`** This file contains variables from the CoronaNet government response project, representing national and sub-national policy event data from more than 190 countries since December 31st, 2019. The data include source links, descriptions, targets (i.e. other countries), the type and level of enforcement, and a comprehensive set of policy types. For more detail on this data, you can see our [codebook here](https://docs.google.com/document/d/1zvNMpwj0onFvUZ_gLl4RRjqS-clbHv3TIX6EOHofsME).

2. **`coronanet_release_allvars.csv`** This file contains the government response information from `coronanet_release.csv` along with the following datasets:

    a. Tests from the CoronaNet testing database (See http://coronanet-project.org for more info);
    b. Cases/deaths/recovered from the [JHU data repository](https://github.com/CSSEGISandData/COVID-19);
    c. Country-level covariates including GDP, V-DEM democracy scores, human rights indices, power-sharing indices, and press freedom indices from the [Niehaus World Economics and Politics Dataverse](https://niehaus.princeton.edu/news/world-economics-and-politics-dataverse)
    
## `coronanet_release.csv` Field Dictionary {-}

1. `record_id` Unique identifier for each unique policy record
2. `policy_id` Identifier linking new policies with subsequent updates to policies
3. `recorded_date` When the record was entered into our data 
4. `date_announced` When the policy is announced
5. `date_start` When the policy goes into effect
6. `date_end` When the policy ends (if it has an explicit end date)
7. `entry_type` Whether the record is new, meaning no restriction had been in place before, or an update (restriction was in place but changed). Corrections are corrections to previous entries.
8. `event_description` A short description of the policy change
9. `type` The category of the policy
10. `type_sub_cat` The sub-category of the policy (if one exists)
11. `type_text` Any additional information about the policy type (such as the number of ventilators/days of quarantine/etc.)
12. `country` The country initiating the policy
13. `init_country_level` Whether the policy came from the national level or a sub-national unit
14. `province` Name of sub-national unit
15. `target_country` Which foreign country a policy is targeted at (i.e. travel policies)
16. `target_geog_level` Whether the target of the policy is a country as a whole or a sub-national unit of that country
17. `target_region` The name of a regional grouping (like ASEAN) that is a target of the policy (if any)
18. `target_province` The name of a province targeted by the policy (if any)
19. `target_city` The name of a city targeted by the policy (if any)
20. `target_other` Any geographical entity that does not fit into the targeted categories mentioned above
21. `target_who_what` Who the policy is targeted at
22. `target_direction` Whether a travel-related policy affects people coming in (Inbound) or leaving (Outbound)
23. `travel_mechanism` If a travel policy, what kind of transportation it affects
24. `compliance` Whether the policy is voluntary or mandatory
25. `enforcer` What unit in the country is responsible for enforcement
26. `link` A link to at least one source for the policy
27. `ISO_A3` 3-digit ISO country codes
28. `ISO_A2` 2-digit ISO country codes
<!-- 22. `severity_index_5perc` 5% posterior low estimate (i.e. lower bound of uncertainty interval) for severity index -->
<!-- 23. `severity_index_median` posterior median estimate (point estimate) for severity index, which comes from a Bayesian latent variable model aggregating across policy types to measure country-level policy severity (see paper on our website) -->
<!-- 24. `severity_index_5perc` 95% posterior high estimate (i.e. upper bound of uncertainty interval) for severity index -->

## `coronanet_release_allvars.csv` Field Dictionary {-}

1. All of the fields listed above, plus
2. `tests_daily_or_total` Whether a country reports the daily count of tests a cumulative total
3. `tests_raw` The number of reported tests collected from host country websites or media reports
4. `deaths` The number of COVID-19 deaths, aggregated to the country-day level (JHU CSSE data)
5. `confirmed_cases` The number of confirmed cases of COVID-19, aggregated to the country-day level (JHU CSSE data)
6. `recovered` The number of recoveries from COVID-19,  aggregated to the country-day level (JHU CSSE data)
7. `ccode` The Correlates of War country code
8. `ifs` IMF IFS country code

9. `Rank_FP` (most recent year available from Niehaus dataset) Reporters without Borders Press Freedom Annual Ranking
10. `Score_FP` (most recent year available from Niehaus dataset) Reporters with Borders Press Freedom Score
11. `state_IDC` (most recent year available from Niehaus dataset) State/Provincial Governments Locally Elected
12. `muni_IDC` (most recent year available from Niehaus dataset) Municipal Governments Locally Elected
13. `dispersive_IDC` (most recent year available from Niehaus dataset) Dispersive Powersharing 
14. `constraining_IDC` (most recent year available from Niehaus dataset) Constraining Powersharing 
15. `inclusive_IDC` (most recent year available from Niehaus dataset) Inclusive powersharing 
16. `sfi_SFI` (most recent year available from Niehaus dataset) State fragility index
17. `ti_cpi_TI` (most recent year available from Niehaus dataset) Corruption perceptions index
18. `pop_WDI_PW` (most recent year available from Niehaus dataset) World Bank population
19. `gdp_WDI_PW` (most recent year available from Niehaus dataset) World Bank GDP (total)
20. `gdppc_WDI_PW` (most recent year available from Niehaus dataset) World Bank GDP per capita
21. `growth_WDI_PW` (most recent year available from Niehaus dataset) World Bank GDP growth percent
22. `lnpop_WDI_PW` (most recent year available from Niehaus dataset) Log of World Bank population
23. `lngdp_WDI_PW` (most recent year available from Niehaus dataset) Log of World Bank GDP
24. `lngdppc_WDI_PW` (most recent year available from Niehaus dataset) Log of World Bank GDP per capita
25. `disap_FA` (most recent year available from Niehaus dataset) 3 category, ordered variable for disappearances index
26. `polpris_FA` (most recent year available from Niehaus dataset) 3 category, ordered variable for political imprisonment index
27. `latentmean_FA` (most recent year available from Niehaus dataset) the posterior mean of the latent variable index for human rights protection)
28. `transparencyindex_HR` (most recent year available from Niehaus dataset) Transparency Index
29. `EmigrantStock_EMS` (most recent year available from Niehaus dataset) Total emmigrant stock from
30. `v2x_polyarchy_VDEM` (most recent year available from Niehaus dataset) Electoral democracy index
31. `news_WB` (most recent year available from Niehaus dataset) Daily newspapers (per 1,000 people)


## Appendix B: Research Assistant Training and Management {-}


## RA Training


All RAs watch a mandatory 50 minute video training of the survey instrument which explains how to use the survey instrument. RAs are also provided with written guidelines on how to collect data and a comprehensive codebook. To briefly describe it here, the written guidelines provide a definition of what counts as a new or updated policy (see Data section for more details) and provides a checklist for RAs to follow in order to identify and document different policies. In the checklist, RAs are instructed to find policies by checking the sources in the order given in the guidelines to identify policies, to document the relevant information into the survey and to save and upload a document of the source they found for each policy into Qualtrics. The codebook meanwhile provides descriptions and examples of the different possible response options in the survey. Using a training video and the written codebook also has the added benefit of helping us efficiently disseminate the information RAs need to use the survey experiment consistently. 

In order to participate as an RA in this project, RAs must fill out a form^[See here for the [link to the form](https://docs.google.com/forms/d/e/1FAIpQLSeybAW0DC0UE1x2EqLiTifVFuSUxqJLGFB8VI4wVCG61tVYKg/viewform).] in which:

* They identify themselves.
* They certify that they have viewed the training video in which we explain how to use the survey instrument.
* They certify they have joined the CoronaNet Slack Channel (see section below for more information).
* They certify that they understand that RA responsibilities entail 
  + gathering historical data on COVID-19 government policy actions for their country, and;
  + providing daily updates for new government policy actions.
* They certify that they understand they can access the data collection guidelines and codebook or pose their questions on the Slack Channel.
* They certify that they are expected to upload .pdfs of the sources they access to the survey instrument.

Once the RA submits the form, they are sent a personalized link to access the survey. With the customized link, we are also able to keep track of which RA coded what entries.

## Real-Time Communication and Feedback

Once an RA joins the project, they can pose their questions on a CoronaNet Slack channel, which they must join in order to participate in the project. The channel allows any RA to pose a question or issue they may have in using the survey instrument to any of the PIs and allows all other RAs to learn from the exchange at the same time. As such, RAs are able to receive feedback and learn from each other's questions in a timely and centralized manner. Since the data collection effort was launched on March 28, 2020 until April 18, 2020, both RAs and PIs have actively used Slack to communicate with one another.  On the Slack channel devoted to asking questions about the Qualtrics data survey in particular, there were 1,752 messages posted by 130 project members. 

<!---To provide a better sense of the level of activity on Slack, Figure \@ref(fig:members) plots the number of project members who had logged into the Slack per day as well as the number of project members who posted at least one message per day and shows that participation and activity Slack  has steadily grown over time.^[Note, the dip in the overall trend corresponds to weekend days.]


```{r members, fig.cap="Plot of the number of Project Members present and active on Slack over time" }

ggplot(slack %>% filter(var %in% c("Project members present on Slack", "Project members posting at least one message" )),
			aes(x = Date, y = value, color = var))+
			geom_line(size = 2)+ 
			xlab("Date")+
			ylab("Number of Project members")+
			scale_x_date(date_labels = "%a %d-%m-%y", date_breaks = "day")+
			scale_color_manual(values=c( "#3B7EA1",  "#FDB515"))+
			theme_minimal()+
			theme(legend.position="bottom",
				  legend.title = element_blank(),
				  axis.text.x = element_text(angle = 90, hjust = 1),
				  panel.grid.minor = element_blank())

```
--->


## Appendix C: List of Contributors to Dataset {-}

```{r ratable}

contribution <- read_csv("../data/CoronaNet/People/contribution_clean.csv")


contribution <- dplyr::rename(contribution, Country = "country") %>% 
  mutate(Vita=coalesce(Vita,""),
         Affiliation=coalesce(Affiliation,"")) 
knitr::kable(contribution,booktabs=T,longtable=T,
          caption = 'Contributing Researchers and their Responsible Countries') %>% 
  kable_styling(latex_options = c("striped", "hold_position")) %>% 
  column_spec(2:3,width="2cm") %>% 
  column_spec(4,width="3cm")


```


# References






