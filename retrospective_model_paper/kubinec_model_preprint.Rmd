---
title: "A Retrospective Bayesian Model for Measuring Covariate Effects on Observed COVID-19 Test and Case Counts"
author: 
  - Robert Kubinec:
      email: rmk7@nyu.edu
      institute: nyuad
      correspondence: true
  - Luiz Max Carvalho:
      institute: gvf
institute:
  - nyuad: New York University Abu Dhabi
  - gvf: School of Applied Mathematics, Getulio Vargas Foundation
date: "April 20th, 2020"
toc: false
output: 
  bookdown::pdf_document2:
    keep_tex: true
    includes:
      in_header:
          preamble.tex
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
  #bookdown::word_document2
bibliography: BibTexDatabase.bib
abstract: "As the COVID-19 outbreak progresses, increasing numbers of researchers are examining how an array of factors either hurt or help the spread of the disease. Unfortunately, the majority of available data, primarily confirmed cases of COVID-19, are widely known to be biased indicators of the spread of the disease. In this paper we present a retrospective Bayesian model that is much simpler than epidemiological models of disease progression but is still able to identify the effect of covariates on the historical infection rate. The model is validated by comparing our estimation of the count of infected to projections from expert surveys and extant disease forecasts. To apply the model, we show that as of April 20th, there are approximately 3 million infected people in the United States, and these people are increasingly concentrated in states with more wealth, better air quality, fewer smokers, more residents under the age of 18, more public health funding and a history of more cardiovascular deaths. On the other hand, the timing of state declarations of emergency and the proportion of people who voted for President Trump in 2016 are not clear predictors of COVID-19 trends. In addition, we find that the US states have increased testing at approximately the same level in line with infections, suggesting that testing has not yet increased significantly above infection trends.^[To reproduce the model and to access the underlying Stan code, please see our [Github page](https://github.com/saudiwin/corona_tscs). This paper is part of the [CoronaNet project](https://lumesserschmidt.github.io/CoronaNet/) collecting data on government responses to the COVID-19 pandemic. For helpful comments we thank Cindy Cheng and Joan Barcelo.]"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE)
require(dplyr)
require(tidyr)
require(ggplot2)
require(rstan)
require(stringr)
require(lubridate)
require(bayesplot)
require(historydata)
require(readr)
require(datasets)
require(extraDistr)
require(patchwork)
require(rstanarm) # for observed data modeling
require(tidycensus)
require(RcppRoll)
require(readxl)
require(ggrepel)
require(missRanger)
# update this package /w data
remotes::install_github("kjhealy/covdata")
require(covdata) # need to install from github repo remotes::install_github("kjhealy/covdata")

set.seed(662817)

rstan_options(auto_write=T)

knitr::opts_chunk$set(warning=F,message=F)

system2("git",args=c("-C ~/covid-tracking-data","pull"))
system2("git",args=c("-C ~/covid-19-data","pull"))

# whether to run model (it will take a few hours) or load saved model from disk

run_model <- F

pan_model <- stan_model("corona_tscs_betab.stan")

```


\newpage



As more and more data has become available on observed case counts of the SARS-CoV2 coronavirus, there have been increasing attempts to infer how contextual factors like government policies, partisanship, and temperature affect the disease's spread [@carleton2020;@sajadi2020;@dudel2020;@tansim2020;@flaxman2020;@brze2020]. The temptation to make inferences from the observed data, however, can result in misleading conclusions. For example, some policy makers have publicly questioned whether the predictions of epidemiological models are far worse than the observed case count.^[See article available at https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d] By contrast, in this paper we show that the unobserved infection rate is a confounding variable affecting any estimates of covariates on the observed counts of COVID-19 cases and tests. For this reason, in this paper we present a retrospective Bayesian model that can adjust for this bias by estimating the unseen infection rate up to an unidentified constant. Furthermore, by incorporating informative priors from the susceptible-infected-recovered (SIR)/susceptible-exposed-infected-recovered (SEIR) papers on SARS-CoV2  [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], it is possible to put an informative prior on the unobserved infection rate and estimate both recent disease trends and the effect of covariates on the historical spread of the disease.

We also show how the model can be applied by measuring the association between U.S. state-level factors and the disease as of May 24, 2020, including the the duration and timing of stay-at-home orders and Google mobility data. We find that, when controlling for stay-at-home orders, increasing vote share for Donald Trump in the 2016 elections by 10 percentage points is associated with an increase of the marginal infection rate of 0.004% (HPD ). In addition, the number of cardiovascular deaths and air particle PM$_2.5$ concentrations positively predict infection rates, while higher rates of smoking, number of health providers, youth and foreign-born residents are negatively associated with infection rates.

To examine the effect of lockdowns, we implement mediation analysis to enable us to see the direct effect of lockdowns on the infection rate versus the indirect of lockdowns via Google mobility data on the infection rate. We include the number of days after the first reported in-state COVID-19 case as an additional moderator. Our analysis shows that the controlled direct effect of lockdowns is heavily conditioned by how long the lockdown was imposed following the first COVID-19 case, with later lockdowns far less effective than earlier lockdowns.

The mediation analysis further shows that the effect of lockdowns through decreasing retail and workplace traffic is strongly depressing of infection rates. Furthemore, the indirect effects are not moderated by the date the lockdown was imposed, suggesting that later lockdowns have been less effective because they have been less able to control mobility. Paradoxically, the effect of lockdowns through transit mobility has been positive, though it is not clear why this may be the case.

# Methods

In this section we present an intuitive overview of the model, and we refer the interested reader to the supplemental materials for a more complete exposition combined with Monte Carlo simulations showing recovery of the latent infection rate. 

Compartmental models employed by epidemiologists to study disease, and in particular SARS-CoV2 [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], suppose different classes (compartments) of individuals in the population, denoted $S$ for susceptible, $I$ for infectious, and $R$ for removed (other compartments may be added such as $E$ for exposed). The model is usually written in the form of a system of ordinary differential equations (ODEs) and assumes a fixed population size, as seems reasonable during a relatively quick epidemic.
The number infected individuals can then be obtained from the solution of the ODE system for the $I$ compartment.
These models guide our understanding of the disease and its progression, and have made warnings about the disease's spread that are proving true on a daily basis.

By contrast, this paper endeavors to estimate a much simpler quantity than the entire evolution of the outbreak. Many researchers and the general public often want to learn about what has already happened, or the *empirical* infection rate (also called the attack rate in the epidemiological literature). For a number of time points $t \in T$ since the outbreak's start and countries/regions $c \in C$, we aim to identify the following quantity:

$$
f_t \left (\frac{I_{ct}}{S_{ct}+R_{ct}} \right )
$$

Assuming a fixed population size, this quantity is simply the marginal rate of infections in the population up to the present.
The function $f_t$ determines the historical time trend of the rate of infection (which is assumed to be same across countries/regions) in the population up to time $T$, the present. Because the denominator is shifting over time due to disease progression dynamics, this model is only useful for retrospection, i.e., to examine factors that may be influencing the empirical time trend $f_t$. As $S_{ct}$ and $R_{ct}$ are exogenous to the model, the model cannot predict future prevalence of the disease given that it does not determine these crucial factors. In other words, this model can be seen as a local linear approximation to the $I_{ct}$ curve from an SIR model.

However, we do not have estimates of the actual infected rate $I_{ct}$, only positive COVID-19 cases $a_{ct}$ and numbers of COVID-19 tests $q_{ct}$. Given this limitation, the aim of the model is to backwards infer the infection rate $I_{ct}$ as a latent process given observed test and counts. Modeling the latent process is necessary to avoid bias in using only observed case counts as a proxy for $I_{ct}$. The reason for this is shown in Figure \ref{tikzfig} in which a covariate $X_{ct}$, such as temperature, is hypothesized to affect the infection rate $I_{ct}$. Unfortunately, increasing infection rates can cause both increasing numbers of observed counts $a_{ct}$ and tests $q_{ct}$. As more people are infected, more tests are likely to be done, which will increase the number of cases independently of the infection rate. As a result, due to the back-door path from the infection rate $I_{ct}$ to case counts $a_{ct}$ via the number of tests $q_{ct}$, it is impossible to infer the effect of $X_{ct}$ on $I_{ct}$ from the observed data alone without modeling the latent infection rate. 

\begin{figure}
\label{tikzfig}
\caption{Directed Acyclic Graph Showing Confounding of Covariate $X_{ct}$ on Observed Tests $q_{ct}$ and Cases $a_{ct}$ Due to Unobserved Infection Rate $I_{ct}$}
\ctikzfig{policy_dag}
\footnotesize{Figure shows the relationship between a covariate $X_{ct}$ representing a policy or social factor influencing the infection rate $I_{ct}$. Because the infection rate $I_{ct}$ influences both the number of reported tests $q_{ct}$ and reported cases $a_{ct}$, any regression of a covariate $X_{ct}$ on the reported data will be biased.}
\end{figure}

The Bayesian model presented in the supplementary materials provides a full explanation of how to model the infection rate's influence on both cases and tests simultaneously. We further show in the supplementary materials that two restrictions are necessary to identify the sign and rank of the effect of covariates $X_{ct}$ on $I_{ct}$: the paths $I_{ct} \rightarrow q_{ct}$ and $I_{ct} \rightarrow a_{ct}$ must be strictly positive so that an increasing infection rate will have a non-decreasing effect on cases and tests. Given these restrictions and weakly informative priors on the parameters, it is possible to know whether $X_{ct}$ is associated with increasing or decreasing $I_{ct}$, though not with respect to the actual number of infected people, only in terms of the covariate's sign or rank relative to other covariates.

Furthermore, we show in the supplementary materials that if further prior information can be put on the ratio between the true number of infected individuals $I_{ct}$ and the number of tests $q_{ct}$, it is possible to transform inferences from the model to approximate counts of infected people up to the present. Thankfully, this information is available through epidemiological models of the disease, which offer inferences on the number of un-diagnosed cases [@li2020;@peak2020]. Based on these estimates, we can put an informative prior in the model that the number of tests is likely to be at least 10% of the total number of infected individuals, reaching as high as 10 times the number of infected.   With this prior information from disease simulations, it is possible to obtain an empirical estimate of infected rates and covariate effects that are more useful to the general public than solely observed tests and cases, as we demonstrate in the next section.

Finally, we note that an advantage of this framework is providing a way to measured the count of infected adjusting for known biases in the number of tests. By comparing numbers of tests per capita and growth rates in cases across regions, the model is able to backwards infer a likely number of infected individuals in a given area. As such it exploits both within-area and between-area variance to adjust for the biases of imperfect testing.

# Results

The only data required to fit the model, in addition to the covariates of interest, are observed cases and tests for COVID-19 by day. In this section, we fit the model to numbers of COVID-19 case counts on US states and territories provided by [The New York Times](https://github.com/nytimes/covid-19-data). By doing so, we can use the differences in trajectories across states to help identify the effect of state-level covariates on the infection rate. We supplement these observed case counts with testing data by day from the [COVID-19 Tracking Project](https://github.com/COVID19Tracking/covid-tracking-data). The testing data starts at March 4th, so we impute the testing data back in time by assuming that the average case/tests ratio stays the same to the origin of the outbreak. Furthermore, as there are discrepancies where the reported number of tests in some states like New Jersey is less than the total number of cases, we impute the number of tests via the case/test ratio for the sample as a whole.

To analyze the effect of suppression policies, we proxy for preparedness to fight the epidemic by including the number of days after the first COVID-19 case that states of emergencies were declared across U.S. states.^[See this site for dates and relevant sources: https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_2020_coronavirus_pandemic] We also add in the dates that stay-at-home (lockdown) orders were imposed after the first COVID-19 case in a given state. We interact the date of lockdowns with a linear counter for each day of the lockdown to measure the increasing effect of lockdowns over time, and we then interact the linear counter with the number of days after the first COVID-19 case that the lockdown was imposed. 

To better understand the mediating effects of the lockdown, we include Google mobility data^[See https://www.google.com/covid19/mobility/] for retail, residential, parks, workplaces, transit and retail establishments. These estimates are by day and aggregated to the state level. They are measured in terms of an index that is initialized with a value of 100 at the index start on February 15th, 2020. To test for mediation, we include these as predictors of the infection rate, and separately fit a linear model (Normal distribution) with each mobility covariate as an outcome and the lockdown covariates as predictors. 

We further add in state-level data on Donald Trump's vote share for the 2016 election from the MIT Election Lab, a 2019 estimate of state GDP from the Bureau of Economic Analysis, the 2018 percentage of foreign born residents from the U.S. Census Bureau, and 2019 state-level average data on air pollution,^[Defined as average exposure of the general public to particulate matter of 2.5 microns or less (PM$_{2.5}$) measured in micrograms per cubic meter (3-year estimate).] cardiovascular deaths per capita, percentage of residents under age 18, number of dedicated health care providers, public health funding, and smoking rates provided by the United Health Foundation [@unhf2019]. All variables are mean-centered. 

In this section we first fit a partially-identified model without any information about the true number of infected people to demonstrate that it is possible to obtain an estimate of the infection trend, though with substantial uncertainty. We then show how we can use insight from SIR/SEIR models to add in informative prior information on the likely number of infected people and translate the estimates into probable infection rates. We then show that these estimates in fact closely track the predictions of SIR/SEIR models for the U.S. population from March 2020, providing external validity for the method and for these predictions.



```{r munge_data,include=F}

# vote share
# MIT Election Lab
load("../data/mit_1976-2016-president.rdata")

vote_share <- filter(x,candidate=="Trump, Donald J.",
                     party=="republican",
                     writein=="FALSE") %>% 
  mutate(trump=candidatevotes/totalvotes)

# state GDP

state_gdp <- readxl::read_xlsx("../data/qgdpstate0120_0_bea.xlsx",sheet="Table 3") %>% 
  mutate(gdp=Q1 + Q2 + Q3 +Q4)

# US Census data - population & percent foreign-born
# note: you need a Census API key loaded to use this -- see package tidycensus docs

acs_data <- get_acs("state",variables=c("B01003_001","B05002_013"),year=2018,survey="acs1") %>% 
  select(-moe) %>% 
  mutate(variable=recode(variable,
                         B01003_001="state_pop",
                         B05002_013="foreign_born")) %>% 
  spread("variable","estimate") %>% 
  mutate(prop_foreign=foreign_born/state_pop)


# health data

health <- read_csv("../data/2019-Annual.csv") %>% 
  filter(`Measure Name` %in% c("Air Pollution","Cardiovascular Deaths","Dedicated Health Care Provider",
                              "Population under 18 years", "Public Health Funding","Smoking")) %>% 
  select(`Measure Name`,state="State Name",Value) %>% 
  distinct %>% 
  spread(key="Measure Name",value="Value")

merge_names <- tibble(state.abb,
                      state=state.name)

# google mobility data

goog_mobile <- read_csv("../data/Global_Mobility_Report.csv",
                        col_types = cols(sub_region_2=col_character())) %>% filter(sub_region_1 %in% merge_names$state,
                                                   is.na(sub_region_2),
                                                   country_region=="United States") %>% 
  rename(state=sub_region_1,
         retail="retail_and_recreation_percent_change_from_baseline",
         grocery="grocery_and_pharmacy_percent_change_from_baseline",
         parks="parks_percent_change_from_baseline",
         transit="transit_stations_percent_change_from_baseline",
         workplaces="workplaces_percent_change_from_baseline",
         residential="residential_percent_change_from_baseline")

# impute some of this data with random forests / some missingness in parks and retail

goog_mobile <- missRanger(goog_mobile,pmm.k=5L)

nyt_data <- read_csv("~/covid-19-data/us-states.csv") %>% 
  complete(date,state,fill=list(cases=0,deaths=0,fips=0)) %>% 
  mutate(month_day=ymd(date)) %>% 
  group_by(state) %>% 
    arrange(state,date) %>% 
  mutate(cases=floor(c(cases[1:2],roll_mean(cases,n=3))),
         Difference=cases - dplyr::lag(cases),
         Difference=coalesce(Difference,0,0)) %>% 
  left_join(merge_names,by="state")

tests <- read_csv("~/covid-tracking-data/data/states_daily_4pm_et.csv") %>% 
  mutate(month_day=ymd(date)) %>% 
  arrange(state,month_day) %>% 
  group_by(state) %>% 
  # first do 3-day moving average
  mutate(total=floor(c(total[1:2],roll_mean(total,n=3)))) %>% 
  mutate(tests_diff=total-dplyr::lag(total),
         cases_diff=positive-dplyr::lag(positive),
         cases_diff=coalesce(cases_diff,positive),
         cases_diff=ifelse(cases_diff<0,0,cases_diff),
         tests_diff=coalesce(tests_diff,total),
         tests_diff=ifelse(tests_diff<0,0,tests_diff)) %>% 
  select(month_day,tests="tests_diff",total,state.abb="state",recovered)

# merge cases and tests

combined <- left_join(nyt_data,tests,by=c("state.abb","month_day")) %>% 
  left_join(acs_data,by=c("state"="NAME")) %>% 
  filter(!is.na(state_pop))

# add suppression data

emergency <- read_xlsx("../data/state_emergency_wikipedia.xlsx") %>% 
  mutate(emergency=as_date(emergency),
         stayhome=as_date(stayhome),
         stayhome=coalesce(stayhome,lubridate::today() + days(1)),
         emergency=coalesce(emergency,lubridate::today() + days(1))) %>% 
  select(state,emergency,stayhome) %>% 
  mutate(state=substr(state,2,nchar(state)),
         state=recode(state,
                      `rth Carolina`="North Carolina",
                      `rth Dakota`="North Dakota",
                      `Illiis`="Illinois"))


end_stayhome <- read_xlsx("../data/state_emergency_wikipedia.xlsx",sheet = "Sheet1") %>% 
  select(state="State",end_stayhome="Date lifted")

emergency <- left_join(emergency,end_stayhome) %>% 
  mutate(end_stayhome=coalesce(as_date(end_stayhome),lubridate::today() + days(1)))

combined <- left_join(combined,emergency,by="state")

# add in other datasets 

combined <- left_join(combined,health,by="state")
combined <- left_join(combined,select(state_gdp,state,gdp),by="state")
combined <- left_join(combined,select(vote_share,state,trump))
combined <- left_join(combined,select(goog_mobile,state,month_day="date",retail:residential))

# impute data

combined <- group_by(combined,state) %>% 
  mutate(test_case_ratio=sum(tests,na.rm=T)/sum(Difference,na.rm=T)) %>% 
  ungroup %>% 
  mutate(test_case_ratio=ifelse(test_case_ratio<1 | is.na(test_case_ratio),
                                mean(test_case_ratio[test_case_ratio>1],na.rm=T),test_case_ratio)) %>% 
  group_by(state) %>% 
    mutate(tests=case_when(Difference>0 & is.na(tests)~Difference*test_case_ratio,
                    Difference==0~0,
                    Difference>tests~Difference*test_case_ratio,
                    Difference==tests~Difference*test_case_ratio,
                    TRUE~tests),
           gdp=gdp/state_pop,
           Difference=ifelse(Difference<0,0,Difference)) %>% 
  arrange(state) %>% 
  filter(state!="Puerto Rico")

combined <- group_by(combined,state) %>% 
  arrange(month_day) %>% 
  mutate(outbreak=as.numeric(cases>1)) %>% 
  fill(outbreak,.direction="down") %>% 
  mutate(outbreak_time=cumsum(outbreak)) %>% 
  ungroup %>%
  mutate_at(c("Cardiovascular Deaths",
              "outbreak_time",
              "Air Pollution",
              "Dedicated Health Care Provider",
              "Smoking",
              "Population under 18 years",
              "Public Health Funding",
              "gdp",
              "grocery",
              "parks",
              "residential",
              "retail",
              "transit",
              "workplaces",
              "trump",
              "prop_foreign"), ~as.numeric(scale(.,scale = F))) %>% 
  group_by(month_day) %>% 
  mutate(world_infect=sum(outbreak_time>1)) %>% 
  group_by(state) %>% 
  arrange(state,month_day) %>% 
  mutate(lockdown=as.numeric(month_day>stayhome & month_day<end_stayhome),
         outbreak=cumsum(cumany(Difference>1)),
         emer_outbreak=month_day[outbreak==1] - month_day[month_day==emergency],
         lockdown_outbreak=ifelse(length(month_day[outbreak==1] - month_day[month_day==stayhome])==0,
                                  month_day[outbreak==1] - max(month_day),
                                  month_day[outbreak==1] - month_day[month_day==stayhome]),
         lockdown=ifelse(lockdown>0,cumsum(lockdown),lockdown)) %>% 
  filter(!is.na(grocery))

# look at how days after lockdown versus days after emergency compare

combined %>% 
  ungroup %>% 
  filter(month_day==max(month_day)) %>% 
  distinct(cases,state,lockdown_outbreak,emer_outbreak,state_pop) %>% 
  ggplot(aes(y=lockdown_outbreak,
             x=emer_outbreak)) +
  geom_point(aes(size=cases/state_pop),colour="red",alpha=0.5) +
  geom_text_repel(aes(label=state)) +
  theme(panel.grid = element_blank(),
        panel.background = element_blank()) + 
  xlab("How Many Days Before the First COVID Case Was a State of Emergency Declared?") +
  ylab("How Many Days Before the First COVID Case Was a Stay at Home Order Imposed?") +
  ggtitle("Comparison of U.S. State Responses to First COVID-19 Case",
          subtitle="Negative Numbers Indicate Policy Was Implemented After First COVID-19 Case")

ggsave("check_scatter.png",width=8,height=6)


saveRDS(combined,"combined.rds")

# create case dataset

cases_matrix <- select(combined,Difference,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(Difference=as.integer(mean(Difference))) %>% 
  spread(key = "month_day",value="Difference")

saveRDS(cases_matrix,"cases_matrix.rds")

cases_matrix_num <- as.matrix(select(cases_matrix,-state))

# create tests dataset

tests_matrix <- select(combined,tests,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(tests=as.integer(mean(tests))) %>% 
  spread(key = "month_day",value="tests")

tests_matrix_num <- as.matrix(select(tests_matrix,-state))

# need the outbreak matrix

outbreak_matrix <- as.matrix(lapply(1:ncol(cases_matrix_num), function(c) {
  if(c==1) {
    outbreak <- as.numeric(cases_matrix_num[,c]>0)
  } else {
    outbreak <- as.numeric(apply(cases_matrix_num[,1:c],1,function(col) any(col>0)))
  }
  tibble(outbreak)
}) %>% bind_cols)

colnames(outbreak_matrix) <- colnames(cases_matrix_num)

time_outbreak_matrix <- t(apply(outbreak_matrix,1,cumsum))

just_data <- select(ungroup(combined),month_day,state,state_pop,trump,air="Air Pollution",
                      heart="Cardiovascular Deaths",
                      providers="Dedicated Health Care Provider",
                      young="Population under 18 years",
                      smoking="Smoking",
                      gdp,
                      public_health="Public Health Funding",
                      prop_foreign,
                      emer_outbreak,lockdown_outbreak,lockdown) %>% arrange(state,month_day) %>% 
  mutate(lockdown_outbreak=as.numeric(lockdown_outbreak),
         int_lockdown=lockdown_outbreak*lockdown,
         emer_outbreak=as.numeric(emer_outbreak))

covs <- select(ungroup(just_data),-state,-state_pop,-month_day) %>% as.matrix

mobility <- select(ungroup(combined),month_day,state,retail:residential) %>% arrange(state,month_day)

covs_mob <- select(ungroup(mobility),-state,-month_day) %>% as.matrix

lockdown <- select(ungroup(just_data),state,month_day,emer_outbreak:int_lockdown) %>% arrange(state,month_day)

covs_lock <- select(ungroup(lockdown),-state,-month_day)


# now give to Stan

ortho_time <- poly(scale(1:ncol(cases_matrix_num)),degree=3)

time_outbreak_center <- matrix(scale(c(time_outbreak_matrix),scale = F),nrow=nrow(time_outbreak_matrix),
                                                   ncol=ncol(time_outbreak_matrix))

real_data <- list(time_all=ncol(cases_matrix_num),
                 num_country=nrow(cases_matrix_num),
                 S=ncol(covs),
                 G=ncol(covs_mob),
                 L=ncol(covs_lock),
                 country_pop=unique(floor(just_data$state_pop/100)),
                 cases=cases_matrix_num,
                 ortho_time=ortho_time,
                 phi_scale=.01,
                 count_outbreak=as.numeric(scale(apply(outbreak_matrix,2,sum),scale = F)),
                 tests=tests_matrix_num,
                 time_outbreak=time_outbreak_matrix,
                 time_outbreak_center=time_outbreak_center,
                 suppress=covs,
                 mobility=covs_mob,
                 lockdown=covs_lock)

saveRDS(real_data,"real_data.rds")

init_vals <- function() {
  list(phi_raw=c(100,100),
       world_infect=0.5,
       finding=0.5,
       country_test_raw=rep(.5,real_data$num_country),
       alpha=c(-1,-1,-1))
}



if(run_model) {
  # loop over states
      
  us_fit <- sampling(pan_model,data=real_data,chains=3,cores=3,iter=800,warmup=500,init=0)


  
  saveRDS(us_fit,"../data/us_fit.rds")
} else {
  us_fit <- readRDS("../data/us_fit.rds")
}


```

```{r infectstate,fig.cap="5% to 95% HPD Uncertainty Intervals of Partially-Identified Infection Rates by U.S. State with Total Average"}
all_est_state <- as.data.frame(us_fit,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                                suppress_measures=covs[1:nrow(cases_matrix),"emer_outbreak"],by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")


all_est_state %>% 
  mutate(estimate=estimate) %>% 
  group_by(state_num,time_point,suppress_measures) %>% 
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  #geom_line(aes(group=state_num,colour=suppress_measures),alpha=0.5) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est,
  group=state_num,
  fill=suppress_measures),alpha=0.5) +
  stat_smooth(colour="black") +
  theme_minimal() +
  scale_color_distiller(palette="RdBu",direction=-1) +
  ylab("Latent Infection Scale") +
  labs(caption="5% - 95% HPD Intervals are colored by\nwhen a state declared a state of emergency (standardized count of days).\nAs the total number of infected people is unknown,\nthis chart measures the relative marginal infection rates between states.") +
  xlab("Days Since Outbreak Start") +
  geom_hline(yintercept = 0,linetype=3) +
  guides(fill=guide_colorbar(title="Timing of State Emergency Declaration")) +
  theme(panel.grid = element_blank(),
        legend.position = "bottom")
ggsave("uncertain_state_rates.png")

```

```{r rankcountries1}

# rank1 <- all_est_state %>%
#   group_by(state,iter) %>%
#   summarize(estimate=mean(estimate)) %>%
#   ungroup %>%
#   group_by(iter) %>%
#   mutate(rank_state=51 - rank(estimate),
#          model="Partially Identified") %>%
#   group_by(state,model) %>%
#     summarize(med_est=mean(rank_state),
#             high_est=quantile(rank_state,.95),
#             low_est=quantile(rank_state,.05))


```

Figure \@ref(fig:infectstate) shows the 5% - 95% high posterior density (HPD) intervals of the latent infection rate by state since January 1st. The intervals are shaded by the relative time when a state declared a state of emergency, which reveals that state of emergency declarations are correlated with higher infection rates. As can be seen, there is a sharp discontinuity in the plot around March 1st when infection rates began to increase. Because the latent scale is not identified, the figure is only showing how the infection rates have evolved from zero to the true but unknown top infection rate. 


```{r suppress2,echo=F,include=F}

suppress_effect <- as.data.frame(us_fit,"suppress_effect") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="parameter",value="estimate",-iter) %>% 
  mutate(variable=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")))

num_infected <- as.data.frame(us_fit,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

# iterate this bugger

p_infected <- group_by(num_infected,iter) %>% 
  summarize(est=mean(dlogis(estimate)))

# don't need to iterate over time anymore, at least for this analysis
if(run_model) {
    over_all <- parallel::mclapply(sample(1:max(num_infected$iter),100), function(q) {
    over_supp <- lapply(1:max(suppress_effect$variable), function(s) {
      # over_time_level <- lapply(seq(min(time_outbreak_center),max(time_outbreak_center),length.out=100), function(i) {
        this_eff <- p_infected$est[q] * suppress_effect$estimate[suppress_effect$variable==s & suppress_effect$iter==q] 
                                         # suppress_effect$estimate[suppress_effect$supp_type==2 & suppress_effect$variable==s & suppress_effect$iter==q]*i)
      
      tibble(estimate=this_eff,
             # time_scale_point=i,
             iter=q,
             variable=s)
      # }) %>% bind_rows
      
    }) %>% bind_rows
    
    return(over_supp)
  },mc.cores=3) %>% bind_rows
    
    saveRDS(over_all,"../data/over_all_nonid.rds")
    
} else {
  over_all <- readRDS("../data/over_all_nonid.rds")
}




# over_all_time <- group_by(over_all,variable,time_scale_point) %>% 
#   summarize(med_est=median(estimate),
#             high_est=quantile(estimate,.95),
#             low_est=quantile(estimate,.05)) %>% 
#   mutate(model="Partially\nIdentified")


over_all_sum <- group_by(over_all,variable) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Partially\nIdentified")
  
  

# calculate marginal effects

# over_all_time <- left_join(over_all_time,tibble(variable=1:ncol(covs),
#                                         label=colnames(covs))) %>% 
#   mutate(label=recode(label,
#                       air="Air\nQuality",
#                       providers="No.\nProviders",
#                       gdp="GDP",
#                       heart="Cardiovascular\nDeaths",
#                       day_emergency="Date\nEmergency",
#                       young="% Population\n<18",
#                       smoking="% Smokers",
#                       trump="Trump\nVote Share",
#                       prop_foreign="% Foreign-Born",
#                       public_health="Public Health\nFunding"))

over_all_sum <- left_join(over_all_sum,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      emer_outbreak="Date\nEmergency",
                      lockdown_outbreak="Date\nLockdown",
                      lockdown="Lockdown\nDuration",
                      int_lockdown="LockdownXDate Lockdown",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

p1 <- over_all_sum %>%
  ggplot(aes(y=med_est,x=reorder(label,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est),alpha=0.8) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  ggtitle("Constant Effect on\nProportion Infected") +
  facet_wrap(~label,scales="free") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5)) +
  coord_flip()

# p2 <- over_all_time %>%
#  ggplot(aes(y=med_est,x=time_scale_point)) +
#     xlab("Time Since Outbreak Began")  +
#   ylab("Effect on Proportion Infected") +
#   geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
#   geom_line(linetype=2) +
#   ggtitle("Time-Varying Effect on\nProportion Infected") +
#   theme_minimal() +
#   geom_hline(yintercept=0,linetype=3) +
#   theme(panel.grid=element_blank(),
#         plot.title = element_text(size=10,hjust=0.5)) +
#   facet_wrap(~label,scales="free_y")

p1

ggsave("marginal_noid.png")

```

```{r run_over_id_model_scale,include=F}


if(run_model) {
  pan_model_scale <- stan_model("corona_tscs_betab_scale.stan")
  
  us_fit_scale <- sampling(pan_model_scale,data=real_data,chains=3,cores=3,iter=800,warmup=500,
                           init=0)
  
  saveRDS(us_fit_scale,"../data/us_fit_scale.rds")
} else {
  us_fit_scale <- readRDS("../data/us_fit_scale.rds")
}


```


```{r naivemodel,include=F}

# calculate the same info but only with observed data

if(run_model) {
  obs_model <- stan_glm(cbind(combined$Difference,floor(combined$state_pop/100) - combined$Difference) ~ poly(outbreak_time,3) +
                     emer_outbreak + 
                        `Cardiovascular Deaths` +
                        `Air Pollution` +
                        `Dedicated Health Care Provider` +
                        `Smoking` +
                        `Population under 18 years` +
                     prop_foreign +
                        `Public Health Funding` +
                        gdp +
                        trump +
                       retail + 
                       grocery + 
                       parks + 
                       transit + 
                       workplaces + 
                       residential +
                     world_infect + 
                       lockdown*lockdown_outbreak,
                      data=combined,
                 QR=TRUE,
                      family="binomial",
                 chains=1,cores=1)

saveRDS(obs_model,"../data/obs_model.rds")
} else {
  obs_model <- readRDS("../data/obs_model.rds")
}



out_data <- mcmc_intervals_data(obs_model,regex_pars=":") %>% 
  filter(!grepl(x=parameter,pattern="I"))

```



```{r infectscaled,fig.cap="Approximate Total Number of COVID-19 Infected Individuals in the U.S. as of April 20th",fig.height=5}
all_est_state <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress[1:nrow(cases_matrix),"emer_outbreak"],by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

# merge in deaths/recovered

case_count <- left_join(case_count,select(combined,deaths,recovered,month_day,state),
                        c("time_point"="month_day","state")) %>% 
  mutate(recovered=coalesce(recovered,0),
         deaths=coalesce(deaths,0),
         cum_sum_cases = cum_sum_cases - deaths - recovered,
         lag_case=ifelse(dplyr::lag(cum_sum_cases,n=14)<0 & !is.na(dplyr::lag(cum_sum_cases,n=14)),0,
                         coalesce(dplyr::lag(cum_sum_cases,n=14),0)),
         cum_sum_cases = cum_sum_cases - lag_case)

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases),
            all_rec=sum(recovered),
            all_death=sum(deaths))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")

calc_sum <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %>% 
  group_by(state_num,iter) %>% 
  arrange(state_num,time_point) %>% 
  mutate(cum_est=cumsum(estimate)) %>% 
  group_by(time_point,iter,all_cum_sum,all_rec,all_death) %>% 
  summarize(us_total=sum(cum_est) - unique(all_rec) - unique(all_death)) %>%
  group_by(iter) %>% 
  arrange(iter,time_point) %>% 
  mutate(us_total=us_total - coalesce(dplyr::lag(us_total,n=14),0)) %>% 
  #summarize(us_total=sum(cum_est)) %>% 
  group_by(time_point,all_cum_sum) %>% 
  summarize(med_est=quantile(us_total,.5),
            high_est=quantile(us_total,.95),
            low_est=quantile(us_total,.05)) 

max_est <- as.integer(round(calc_sum$med_est[calc_sum$time_point==max(calc_sum$time_point)]))
high_max_est <- as.integer(round(calc_sum$high_est[calc_sum$time_point==max(calc_sum$time_point)]))
low_max_est <- as.integer(round(calc_sum$low_est[calc_sum$time_point==max(calc_sum$time_point)]))
max_obs <- calc_sum$all_cum_sum[calc_sum$time_point==max(calc_sum$time_point)]

options(scipen=999)

# load expert survey results

expert_survey <- read_csv("../data/consensusForecastsDB.csv") %>% 
  filter(questionLabel %in% c("QF5","QF4","QF3"),
         surveyIssued>ymd("2020-03-16")) %>% 
    mutate(keep=case_when(surveyIssued==ymd("2020-03-02")~"QF4",
                        surveyIssued==ymd("2020-03-09")~"QF6",
                        surveyIssued==ymd("2020-03-16")~"QF4",
                        surveyIssued==ymd("2020-03-23")~"QF4",
                        surveyIssued==ymd("2020-03-30")~"QF3",
                        TRUE~"reject")) %>% 
  filter(questionLabel==keep,cumprob>0.05,cumprob<0.95) %>% 
  group_by(surveyIssued,questionLabel) %>% 
  summarize(med_est=bin[abs(cumprob-0.5)==min(abs(cumprob-0.5))],
            low_est=bin[abs(cumprob-0.1)==min(abs(cumprob-.1))],
            high_est=bin[abs(cumprob-0.9)==min(abs(cumprob-.9))]) %>% 
  rename(time_point="surveyIssued")


# need to add in cumulative case counts

calc_sum %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est),
  fill="blue",
  alpha=0.5) +
  geom_line(aes(y=all_cum_sum)) +
  theme_minimal() +
  ylab("Total Number Infected/Reported") +
  scale_y_continuous(labels=scales::comma) +
  labs(caption="Blue 5% - 95% HPD intervals show estimated infected and the black line\nshows observed cases from the New York Times.\nThese estimates are based on the assumption that as few as 10% of cases\nmay be reported based on SIR/SEIR models.") +
  annotate("text",x=ymd(c("2020-04-17","2020-04-17")),
           y=c(max_est-700000,max_obs+300000),
           hjust=1,
           vjust=0,
           fontface="bold",
           size=3,
           label=c(paste0("Estimated Infected:\n",formatC(low_max_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_max_est,big.mark=",",format = "f",digits=0)),
                   paste0("Total Reported Cases:\n",formatC(max_obs,big.mark=",")))) +
  annotate("text",x=ymd(c("2020-03-01","2020-03-12",
                          as.character(expert_survey$time_point))),
           y=c(300000,180000,expert_survey$high_est*1.01),
           vjust=0,
           fontface="bold",
           size=2,
           label=c("Li et al. March 8th",
                   "Perkins et al. March 26th",
                   paste0("Expert Survey\n",expert_survey$time_point)),alpha=0.8) +
  # previously published annotations
  annotate("pointrange",x=ymd("2020-03-01"),y=9001,ymin=2299,ymax=20403,alpha=0.5) +
  annotate("pointrange",x=ymd("2020-03-12"),y=22876,ymin=7451,ymax=53044,alpha=0.5) +
  geom_pointrange(data=expert_survey,aes(ymin=low_est,ymax=high_est),alpha=0.5) +
  xlab("Days Since Outbreak Start") +
  theme(panel.grid = element_blank(),
        legend.position = "top")

ggsave("est_vs_obs_experts.png")

```

By comparison, Figures \@ref(fig:infectscaled) and \@ref(fig:stateplot) show fully-identified models incorporating informative prior information suggesting that the ratio of tests to infected ratio individuals is probably no less than 10% of those infected (though it could very high). This information, as previously mentioned, was derived from simulation and statistical modeling of COVID-19 outbreaks so far suggesting that a large proportion of infected individuals are undetected [@li2020;@peak2020]. Based on this information, the scale of the latent infection process shown in Figure \@ref(fig:infectstate) can be further identified. In this figure, we calculate cumulative counts, and then subtract away reported recovered cases, deaths and a 14-day lag for recoveries from unreported infections (assuming such infections are mild). 

Figure \@ref(fig:infectscaled) shows that the likely present number of infected cases, excluding those who have recovered or died, is likely approaching 4 million presently infected individuals in the United States, in line with SIR/SEIR and expert survey projections released recently.^[See https://www.nytimes.com/2020/04/01/world/coronavirus-news.html?action=click&module=Spotlight&pgtype=Homepage for a recent overview.]  Furthermore, Figure \@ref(fig:stateplot) shows significant state by state heterogeneity, with New York showing the greatest number of infected, followed by California. There is some suggestive evidence that California's infected count growth may be slowing, though the large uncertainty interval suggests that this inference would be unwise without more data or assumptions.

As described in the modeling section, the model does not provide estimates that can be extrapolated into the future. Because the model is estimating empirical infection rates, it is primarily useful for adjusting empirical data, or the count of tests and cases and other background factors. However, because the model is substantively different than the SIR/SEIR simulations used to guide policy choices, it provides helpful external validation of these models incorporating observed information with minimal assumptions.

```{r stateplot,fig.cap="Average Cumulative Count of Infected People by U.S. State as of April 20th",fig.height=5,echo=F}
require(ggrepel)

# need different figures for case-level deaths + recovered

all_est_state <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress[1:nrow(cases_matrix),"emer_outbreak"],by="state_num"))

all_est_state <- left_join(all_est_state,case_count,by=c("state","time_point"))

calc_sum_state <- all_est_state %>% 
  ungroup %>% 
  #mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %>% 
  mutate(estimate=(plogis(estimate))*((state_pop))) %>%
  group_by(state,iter) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_est=cumsum(estimate) - recovered - deaths,
         cum_est=cum_est-coalesce(dplyr::lag(cum_est,n=14),0)) %>% 
  group_by(time_point,state,suppress_measures) %>% 
  summarize(med_est=quantile(cum_est,.5),
            high_est=quantile(cum_est,.95),
            low_est=quantile(cum_est,.05)) 

# Annotations

# get top 5 plus random 5 

top_5 <- filter(calc_sum_state,time_point==max(calc_sum_state$time_point)) %>% 
  arrange(desc(med_est)) %>% 
  ungroup %>% 
  slice(c(1:5,sample(6:length(unique(calc_sum_state$state)),5))) %>% 
  distinct %>% 
  mutate(label=paste0(state,":",formatC(low_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_est,big.mark=",",format = "f",digits=0)))

all <- calc_sum_state %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_line(aes(group=state,colour=med_est)) +
  # geom_ribbon(aes(ymin=low_est,
  # ymax=high_est,
  # group=state_num,
  # fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_distiller(palette="Reds",direction=1) +
  ylab("Current Number Infected") +
  geom_text_repel(data=top_5,aes(x=time_point,y=med_est,label=label),
                  size=3,fontface="bold",segment.colour = NA) +
  scale_y_continuous(labels=scales::comma) +
  xlab("Days Since Outbreak Start") + 
  guides(colour="none") +
  theme(panel.grid = element_blank(),
        legend.position = "top")

# same calculations, but per capita

all_est_state <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress[1:nrow(cases_matrix),"emer_outbreak"],by="state_num"))

all_est_state <- left_join(all_est_state,case_count,by=c("state","time_point"))

calc_sum_state <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=((plogis(estimate))*state_pop)) %>% 
  group_by(state,iter) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_est=cumsum(estimate) - recovered - deaths,
         cum_est=cum_est-coalesce(dplyr::lag(cum_est,n=14),0),
         cum_est=cum_est/(state_pop*100)) %>% 
  group_by(time_point,state,suppress_measures) %>% 
  summarize(med_est=quantile(cum_est,.5),
            high_est=quantile(cum_est,.95),
            low_est=quantile(cum_est,.05)) 

# Annotations

# get top 5 plus random 5 

top_5 <- filter(calc_sum_state,time_point==max(calc_sum_state$time_point)) %>% 
  arrange(desc(med_est)) %>% 
  ungroup %>% 
  slice(c(1:5,sample(6:length(unique(calc_sum_state$state)),5))) %>% 
  distinct %>% 
  mutate(label=paste0(state,":",formatC(low_est*100,big.mark=",",format = "f",digits=1)," - ",
                                                         formatC(high_est*100,big.mark=",",format = "f",digits=1)))


per_cap <- calc_sum_state %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_line(aes(group=state,colour=med_est)) +
  # geom_ribbon(aes(ymin=low_est,
  # ymax=high_est,
  # group=state_num,
  # fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_distiller(palette="Reds",direction=1) +
  ylab("Percent Population Infected") +
  labs(caption="Some lines are labeled with uncertainty of estimates (5% - 95% Interval).\nThese estimates are based on the assumption that as few as\n10% of cases may be reported based on SIR/SEIR models. Does not exclude people\nwho may have recovered or died.") +
  geom_text_repel(data=top_5,aes(x=time_point,y=med_est,label=label),
                  size=3,fontface="bold",segment.colour = NA) +
  scale_y_continuous(labels=scales::percent) +
  xlab("Days Since Outbreak Start") + 
  guides(colour="none") +
  theme(panel.grid = element_blank(),
        legend.position = "top") 

all / per_cap

ggsave("certain_state_rates.png")
```

Figure \@ref(fig:rankcountries2) shows the results of the mediation analysis, in which we examined the indirect effect of lockdowns on each type of Google mobility data, permitting us to also calculate the controlled direct effect of lockdowns. The controlled direct effect is shown in panel A in Figure \@ref(fig:rankcountries2), while the mediated indirect effects are in panel B. The x axis of the plot shows the varying effect of lockdowns by the number of days following the first COVID-19 case in a given state that the lockdown was imposed. Generally speaking, the lockdowns imposed many days after the first COVID-19 case were in states like California and New York that saw cases relatively early, but reacted much later. 

What is clear from examining panel A is that the direct effect of lockdowns, excluding the effect mediated through mobility, is highly negative for states that adopted lockdowns many days after the first COVID-19 case, and quite the opposite for states adopting lockdowns very soon after their first COVID-19 case. In essence, lockdowns appear to be associated with more infections in these states, and do not show as great suppression as in the states that already had significant COVID-19 infections at time of first adoption.

The indirect effects in panel B may provide some insight as to why this is the case. While the indirect effects on grocery stores and parks are close to zero, the effects of the lockdown through retail and workplace are strongly negative. Furthermore, these effects are relatively constant regardless of when the lockdown was imposed. In other words, the increasing direct effect of lockdowns may be because it is having effects apart from the mobility. To the extent that lockdowns keep people away from retail establishments and workplaces, they have a strong supppression effect on the virus.

It is important to note, however, that lockdowns are associated with increases in infections when mediated through transit and residential mobility. Residential mobility is easier to explain as it is well-known that trapping people in homes will increase transmission within the home. Transit mobility, though, is harder to explain, and more research will need to be done to uncover the reasons for this association.

```{r rankcountries2,fig.cap="Mediated Effects of Lockdowns on Google Mobility Data",fig.height=4}

# to calculate these effects, need to marginalize over the 
# sample data, and also the number of mediators/direct effects

mob_effect <- rstan::extract(us_fit_scale,"mob_effect")[[1]]
lock_effect <- rstan::extract(us_fit_scale,"suppress_hier_const")[[1]]
direct_effect <- rstan::extract(us_fit_scale,"suppress_effect")[[1]]

p_infected <- select(num_infected,iter,state_num,time_point,estimate) %>% 
  spread(key="iter",value="estimate") %>% 
  ungroup %>% 
  select(-state_num,-time_point,-variable) %>% 
  as.matrix %>% 
  dlogis

# make a matrix of derivatives

if(run_model) {
  
    num_infected_deriv <- mutate(num_infected,estimate=dlogis(estimate))
    
    over_med <- lapply(1:ncol(covs_mob), function(m) {

      over_mod <- lapply(seq(min(combined$lockdown_outbreak),max(combined$lockdown_outbreak),
                             length.out=10), function(d) {

          tibble(direct_eff=rowMeans((direct_effect[i,12] + direct_effect[i,13]*d)*p_infected),
                  indirect_eff=rowMeans((mob_effect[i,m]*(lock_effect[i,m,3] + lock_effect[i,m,4]*d)*p_infected)),
                 total_eff=direct_eff + indirect_eff,
                 prop_med=indirect_eff/total_eff) %>% 
            mutate(mobility=colnames(covs_mob)[m],
                   lockdown="lockdown",
                   day=d)
          
        }) %>% bind_rows
    }) %>% bind_rows
    
    saveRDS(over_med,"../data/mediator_eff.rds")
    
} else {
  over_med <- readRDS("../data/mediator_eff.rds")
}

over_med_agg <- over_med %>% 
  group_by(mobility,lockdown,day) %>% 
  summarize(med_direct=mean(direct_eff),
            high_direct=quantile(direct_eff,.95),
            low_direct=quantile(direct_eff,.05),
            med_indirect=mean(indirect_eff),
            high_indirect=quantile(indirect_eff,.95),
            low_indirect=quantile(indirect_eff,.05),
            med_total=mean(total_eff),
            high_total=quantile(total_eff,.95),
            low_total=quantile(total_eff,.05)) %>% 
  ungroup %>% 
  select(-lockdown) %>% 
  mutate(mobility=recode(mobility,grocery="Grocery",
                         parks="Parks",
                         residential="Residential",
                         transit="Transit",
                         retail="Retail",
                         workplaces="Workplaces",
                         residential="Residential")) %>% 
  gather(key="type",value="estimate",-mobility,-day) %>% 
  separate(type,into=c("est_type","variable")) %>% 
  spread(key="est_type",value="estimate")

direct <- over_med_agg %>% 
  filter(variable=="direct",mobility=="Residential") %>% 
  ggplot(aes(y=med,x=day*-1)) +
  geom_ribbon(aes(ymin=low,ymax=high),fill="blue",alpha=0.5) +
  geom_line(linetype=2) +
  geom_hline(yintercept = 0) +
  ylab("Marginal % Change in Infected Rate") +
  xlab("Days After First COVID-19 Case Until Lockdown Imposed") +
  scale_y_continuous(labels=scales::percent) 

indirect <- over_med_agg %>% 
  filter(variable=="indirect") %>% 
  ggplot(aes(y=med,x=day*-1)) +
  geom_ribbon(aes(ymin=low,ymax=high),fill="blue",alpha=0.5) +
  geom_line(linetype=2) +
  geom_hline(yintercept = 0) +
    facet_wrap(~mobility) +
  xlab("") +
  ylab("") +
  scale_y_continuous(labels=scales::percent)

total <- over_med_agg %>% 
  filter(variable=="total") %>% 
  ggplot(aes(y=med,x=day*-1)) +
  geom_ribbon(aes(ymin=low,ymax=high),fill="blue",alpha=0.5) +
  geom_line(linetype=2) +
  geom_hline(yintercept = 0) +
  facet_wrap(~mobility) + 
scale_y_continuous(labels=scales::percent)

direct + indirect  +
      plot_annotation(tag_levels = "A",
                      caption="Results from mediation analysis using MCMC with Stan.\nPanel A shows controlled direct effect of lockdowns by number of days after first COVID-19 case lockdown imposed.\nPanel B shows indirect effect of lockdowns mediated through mobility data by number of days after first COVID-19 case lockdown imposed.") & theme(strip.placement = NULL)


```

Figure \@ref(fig:suppress1) shows the marginal effect of covariates in the model on the latent infection rate, expressed as the marginal increase in proportions for a standard deviation increase in the covariate.States with larger young and foreign-born populations, a higher GPD per capita and more health care providers are seeing increasingly higher infection rates. By contrast, states with more smokers and  better air quality are seeing increasingly fewer infections. These set of associations are not necessarily causal, as they are influenced by the spatial spread of the disease thus far, with outbreaks starting in wealthy coastal states and progressively moving inland. As the disease continues, we expect these associations to shift as we learn more about the effect of suppression policies targeting the disease.

However, it is important to note that state-level Trump vote share is associated with increasing COVID-19 infections, following public opinion showing wide partisan disagreement in concern over the virus.^[See https://www.vox.com/2020/3/15/21180506/coronavirus-poll-democrats-republicans-trump.] The marginal effect of 0.0004% is for a one-standard deviation increase in Trump vote share, or approximately ten percentage points. In a state as large as Texas (29 million people), such an increase in vote share would be associated with an approximate 116,000 increase in infections. Assuming a low infected fatality rate of 0.01, as some studies have suggested, that increase in Trump vote share would be associated with an additional 1160 deaths in the state of Texas.

```{r suppress1,echo=F,fig.height=7,fig.cap="Marginal Effects of Covariates on Latent Infection Rates for U.S. States"}

suppress_effect <- as.data.frame(us_fit_scale,"suppress_effect") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="parameter",value="estimate",-iter) %>% 
  mutate(variable=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?"))) %>% 
  filter(variable<10)

# num_infected <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
#   mutate(iter=1:n()) %>% 
#   gather(key="variable",value="estimate",-iter) %>% 
#   group_by(variable) %>% 
#   mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
#          time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
#          time_point=ymd(min(combined$month_day)) + days(time_point-1))
# 
# # iterate this bugger
# 
# 
# p_infected <- group_by(num_infected,iter) %>% 
#   summarize(est=mean(dlogis(estimate)))

if(run_model) {
  
    over_all2 <- lapply(1:max(suppress_effect$variable), function(s) {
      
        this_eff <- (suppress_effect$estimate[suppress_effect$variable==s]*p_infected*sd(covs[,s]))/100 
        this_eff <- rowMeans(this_eff)
      tibble(estimate=this_eff,
             variable=s)
      
    }) %>% bind_rows
    
    saveRDS(over_all2,"../data/over_all2.rds")
    
} else {
  over_all2 <- readRDS("../data/over_all2.rds")
}


over_all_sum2 <- group_by(over_all2,variable) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Fully\nIdentified")
  
  

# calculate marginal effects

# over_all_time2 <- left_join(over_all_time2,tibble(variable=1:ncol(covs),
#                                         label=colnames(covs))) %>% 
#   mutate(label=recode(label,
#                       air="Air\nQuality",
#                       providers="No.\nProviders",
#                       gdp="GDP",
#                       heart="Cardiovascular\nDeaths",
#                       day_emergency="Date\nEmergency",
#                       young="% Population\n<18",
#                       smoking="% Smokers",
#                       trump="Trump\nVote Share",
#                       prop_foreign="% Foreign-Born",
#                       public_health="Public Health\nFunding"))

over_all_sum2 <- left_join(over_all_sum2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="PM 2.5",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

p3 <- over_all_sum2 %>%
  ggplot(aes(y=med_est,x=reorder(label,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est),alpha=0.8) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  ggtitle("Cumulative Effect on\nProportion Infected") +
  geom_text(aes(label=scales::percent(med_est)),vjust=-1) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        strip.text=element_text(face="bold"),
        axis.text.y=element_text(size=8)) +
  scale_y_continuous(labels=scales::percent) +
  coord_flip() 

# p4 <- over_all_time2 %>%
#  ggplot(aes(y=med_est,x=time_scale_point)) +
#     xlab("Time Since Outbreak Began")  +
#   ylab("Effect on Proportion Infected") +
#   labs("Intervals are 5% - 95% high posterior density (HPD) uncertainty intervals.") +
#   geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
#   geom_line(linetype=2) +
#   ggtitle("Time-varying Effect on\nProportion Infected") +
#   theme_minimal() +
#   geom_hline(yintercept=0,linetype=3) +
#   theme(panel.grid=element_blank(),
#         plot.title = element_text(size=10,hjust=0.5),
#         strip.text=element_text(face="bold",size=9),
#         axis.text.y=element_text(size=8)) +
#   facet_wrap(~label,scales="free_y")

p3 +  plot_annotation(caption="Marginal effects calculated as a 1-standard deviation change in a covariate on the\nlatent infection rate. 5% - 95% high posterior density intervals derived from\n100 Markov Chain Monte Carlo posterior draws.")

ggsave("marginal_id.png")

```


```{r compareids,fig.cap="Comparison Of Time-Varying Covariate Marginal Effects from Partially- and Fully-Identified Models",fig.height=8,fig.width=4}

# combine_times <- bind_rows(over_all_time,over_all_time2)
# 
# fac1 <- combine_times %>%
#   filter(model=="Partially\nIdentified") %>% 
#  ggplot(aes(y=med_est,x=time_scale_point)) +
#     xlab("")  +
#   ylab("Effect on Proportion Infected") +
#   geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
#   geom_line(linetype=2) +
#   ggtitle("Partially\nIdentified") +
#   theme_minimal() +
#   geom_hline(yintercept=0,linetype=3) +
#   theme(panel.grid=element_blank(),
#         plot.title = element_text(size=10,hjust=0.5),
#         strip.text=element_blank(),
#         axis.text.y=element_text(size=8)) +
#   facet_grid(vars(label),scales="free_y")
# 
# fac2 <- combine_times %>%
#   filter(model=="Fully\nIdentified") %>% 
#  ggplot(aes(y=med_est,x=time_scale_point)) +
#     xlab("")  +
#   ylab("") +
#   labs("Intervals are 5% - 95% high posterior density (HPD) uncertainty intervals.") +
#   geom_ribbon(aes(ymin=low_est,ymax=high_est),alpha=0.5,fill="red") +
#   geom_line(linetype=2) +
#   ggtitle("Fully\nIdentified") +
#   theme_minimal() +
#   geom_hline(yintercept=0,linetype=3) +
#   theme(panel.grid=element_blank(),strip.text.y=element_text(face="bold",size=8,angle = 0),
#         plot.title = element_text(size=10,hjust=0.5),
#         axis.text.y=element_text(size=8)) +
#   facet_grid(vars(label),scales="free_y")
# 
# fac1 + fac2 +
#   plot_annotation(caption="Time Since Outbreak Began",
#                   theme=theme(plot.caption=element_text(hjust=0.5)))

#gridExtra::grid.arrange(fac1,fac2,ncol=2)
```


<!-- Figure \@ref(fig:rankcountries2) compares the identified and partially-identified models' infection rates by plotting the average posterior rank of infection for each state and each model. As can be seen, while there is stochastic noise, there is a very clear and strong relationship in terms of the ranks, with a linear fit very close to the 45-degree horizontal line indicating a 1:1 relationship. While the models' predictions differ, they are estimating the same latent quantity, albeit with noise. -->

While the use of informative priors is very helpful for obtaining estimates that can be mapped back to actual number of infected persons, as we show in the supplementary information, we can in fact identify covariate effects (or at least their signs) without information about the ratio of tests to infected persons. To demonstrate this, we calculate time-varying marginal effects on the latent infected rate from both the partially-identified and fully-identified models and show them in Figure \@ref(fig:compareids). While there are certainly effect size and uncertainty differences between the models, the estimates are quite similar, and the signs are always in the same direction. For these reasons, we believe this model to be useful even in the case where there is no useful or credible information about the ratio of testing to infected.

Finally, Figure \@ref(fig:compmarg) compares the underlying parameter estimates from the latent Bayesian model and a binomial model of the observed case counts with the same covariates as predictors (including time trends). As can be seen, the estimates of these models can wildly diverge, with the observed case count model showing far larger and implausibly precise associations between covariates and case counts. Of particular worry is when covariates are correlated with a state's ability or willingness to test for the virus.^[We note that testing may be conducted for the virus using PCR, or for the disease, which would also include serology. This also may explain why the observed model in Figure \@ref(fig:compmarg) shows such a high effect of GDP per capita on reducing infection counts.]


For example, the coefficient for public health shows an implausibly large positive association (+8 on the *logit* scale) in the observed cases model, suggesting that the more public health funding in a state, the higher the infection rate. The latent infection model, by contrast, shows a positive but very small effect. To show the potential confounding in this result, Figure \@ref(fig:comptests) plots public health funding against state COVID-19 tests per capita, revealing states with more infected people--Rhode Island and New York--have implemented many tests and are also have more public health funding. As such, the observed data model is likely obfuscating the strength of the correlation between public health funding and the number of tests with the spread of the disease.


```{r compmarg,fig.cap="Comparison of Effects from Latent and Observed Data Models"}

plot_data1 <- mcmc_intervals_data(us_fit,regex_pars ="effect") %>%
  mutate(variable=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?"))) %>% 
  filter(variable<10)

# join back to labels

plot_data1 <- left_join(plot_data1,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>%
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      emer_outbreak="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data2 <- mcmc_intervals_data(us_fit_scale,regex_pars ="suppress\\_effect") %>%
  mutate(variable=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?"))) %>% 
  filter(variable<10)

# join back to labels

plot_data2 <- left_join(plot_data2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>%
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      emer_outbreak="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data3 <- mcmc_intervals_data(obs_model,pars=c("emer_outbreak",
                                                   "trump",
                                                   "`Air Pollution`",
                                                   "`Cardiovascular Deaths`",
                                                   "`Dedicated Health Care Provider`",
                                                   "`Population under 18 years`",
                                                   "Smoking",
                                                   "gdp",
                                                   "prop_foreign")) %>%
  mutate(variable=c(1:9))

# join back to labels

plot_data3 <- left_join(plot_data3,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>%
  mutate(label=recode(label,
                      air="Air\nQuality",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      emer_outbreak="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

plot_data <- bind_rows(list(`Latent\nInfection Rate`=plot_data2,
                            `Observed\nCases`=plot_data3),
                       .id="model")

p1 <- plot_data %>%
  ggplot(aes(y=m,x=reorder(label,m))) +
  geom_pointrange(aes(ymin=ll,ymax=hh,colour=model),alpha=0.8,position=position_dodge(width=0.5),size=.5) +
  theme_minimal() +
  xlab("")  +
  ylab("Parameter Estimate\n(Logit Scale)") +
  geom_hline(yintercept=0,linetype=2) +
  scale_color_brewer(type="qual") +
  ggtitle("Constant Coefficients") +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        legend.position = "top") +
    guides(color=guide_legend(title="")) +
  coord_flip()

p1 +  plot_annotation(caption="Both models were fit with the same covariates and specification, and\nusing the same  Markov Chain Monte Carlo samplers. Parameter values are on the logit scale.\nIntervals are 5% - 95% high posterior density intervals.")

ggsave("effect_compare.png")

```

```{r comptests,fig.cap="Comparison of GDP Per Capita and COVID-19 Tests per 10,000 Residents by State"}

check_tests <- group_by(combined,state,`Public Health Funding`) %>% summarize(mean_tests=(mean(tests,na.rm=T)/state_pop[1])*10000) %>% 
  filter(state!="District of Columbia")

check_tests %>% 
  ggplot(aes(y=`Public Health Funding`,x=mean_tests)) +
    stat_smooth(method="lm",alpha=0.2) +
  geom_text_repel(aes(label=state),alpha=0.8,segment.size=0) +
  theme_minimal() +
  labs("Plot shows some states like New York and Massachusetts have high average income and also\nvery high testing rates.") +
  theme(panel.grid = element_blank()) +
  ylab("Public Health Funding") +
  xlab("Tests per 10,000 state residents")

ggsave("gdp_tests.png")


```

In addition to the estimation of covariates, the model provides further useful information by parameterizing the relationship between the unobserved infection rate and the number of tests conducted in a given state. These individual parameters are shown in Figure \@ref(fig:tpi). The scale of the y axis shows by how much tests will increase as a percentage of the population for a one percent increase in infection rates in each state as of May 24th. The plot shows that some states could increase their tests as a proportion of the population by roughly double this infection rate (New York, Rhode Island), while other states like Texas and Montana are increasing tests at a much slower rate than the growth rate in infections, a worrying sign.


```{r tpi,fig.height=6,fig.cap="Measuring States' Testing Rates Relative to Infection Rates"}

# convert to marginal changes

test_var <- as.data.frame(us_fit_scale,"country_test_raw") %>%
  mutate(iter=1:n()) %>%
  gather(key="variable",value="estimate",-iter) %>%
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")))

this_infect <- filter(ungroup(num_infected),time_point==max(time_point)) %>% 
  left_join(tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop)) %>% 
  mutate(estimate2=qlogis(plogis(estimate)+.01))

test_var <- left_join(test_var,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop))

alpha_test <- as.data.frame(us_fit_scale,pars="alpha[1]")

# loop over infections

over_states <- lapply(unique(test_var$state), function(s) {
  tibble(estimate=plogis((alpha_test$`alpha[1]` + test_var$estimate[test_var$state==s] * this_infect$estimate2[this_infect$state==s])) - plogis((alpha_test$`alpha[1]` + test_var$estimate[test_var$state==s] * this_infect$estimate[this_infect$state==s])),
         state=s)
}) %>% bind_rows



over_states %>%
  group_by(state) %>%
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>%
  ggplot(aes(y=med_est,x=reorder(state,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est)) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  coord_flip() +
  xlab("") +
  scale_y_continuous(labels=scales::percent) +
  # scale_y_continuous(breaks=c(.96,.98,1,1.02,1.04,1.06),
  #                    labels = c("Ahead of\nCurve",".98","Keeping\nUp","1.02","1.04","Behind\nCurve")) +
  # annotate("text",y=c(.96,1.05),x=c("Georgia","North Dakota"),label=c("Testing More\nThan Infection Growth","Testing Less\nThan Infection Growth"),size=3) +
  # geom_hline(yintercept=1,linetype=2) +
  labs(caption = "Scale of the estimates shows how increases in the infection rate increase\nthe number of tests in a given state. Higher numbers indicate fewer tests\nconducted for a unit increase in the infection rate (on the logit scale).") +
  ylab("Additional Proportion Tested of Population for Every Percent Increase in Infected")

ggsave("testing.png",scale=1.1)

```

We would note that this information is also helpful to policy makers and others trying to make sense of observed case counts given the limitation in testing thus far. Our estimates help take into account these known biases and adjust them based on differences between states and within states in terms of disease trajectories. We believe this model can be used to help understand disease trends and factors associated with it even in the relatively data-poor environment many countries find themselves in. 

# Conclusion

This model was devised to permit the identification of suppression measures and social, political and economic factors on the spread of COVID-19. It is not intended to be a replacement or alternative to the disease forecasting literature, especially as this model relies on SIR/SEIR estimates for full identification. If anything, this modeling exercise shows why explicit mechanistic epidemiological models are so important: without them it is literally impossible to know the total number of infected people on a given day. This model's simplicity and ability to use empirical data are its main features, and the hope is that it can be used and extended by researchers looking at government policies and other tertiary factors on the spread of the disease. At the very least, the model provides realistic uncertainty intervals taking into account very real biases in the observed data.

In addition, the model provides insight into how the number of tests undertaken by a given country or area compares to the probably number of infections. These parameter estimates can be used to understand whether a state's testing exceeds, is the same as or is less than the number of infected individuals. Given the wide problem of data scarcity in understanding the disease's spread, we hope this model can be used to make the most of empirical evidence. 

To fit the model, it is necessary to have at least an estimate of how many tests have been conducted. The  [CoronaNet](https://lumesserschmidt.github.io/CoronaNet/) project is currently working to obtain testing data, in addition to information about government policy responses to COVID-19, in an effort to better understand the role and success of variation in country policy responses to date. 

# Bibliography


