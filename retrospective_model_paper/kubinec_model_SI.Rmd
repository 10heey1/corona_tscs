---
title: "Supplementary Information for A Retrospective Bayesian Model for Measuring Covariate Effects on Observed COVID-19 Test and Case Counts"
date: "Septmeber 19, 2020"
output: 
  bookdown::pdf_document2:
    includes: 
      in_header:
          preamble.tex
toc: false
csl: nature.csl
bibliography: BibTexDatabase.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(dplyr)
require(tidyr)
require(ggplot2)
require(rstan)
require(stringr)
require(lubridate)
require(bayesplot)
require(historydata)
require(readr)
require(datasets)
require(extraDistr)
require(rstanarm)

set.seed(662817)

rstan_options(auto_write=T)

knitr::opts_chunk$set(warning=F,message=F)

# whether to run model (it will take a few hours) or load saved model from disk

run_model <- F

```

# Testing and COVID-19

In addition to the estimation of the cumulative count of infected individuals, the model provides further useful information by parameterizing the relationship between the unobserved infection rate and the number of tests conducted in a given state. These individual parameters are shown in Figure \@ref(fig:tpi). The scale of the y axis shows the number of people that a state was able to test relative to each person infected. The plot shows that some states have been able to test far more people than have been infected (New York, Rhode Island), while other states like Texas, Arizona and Pennsylvania have tested barely twice as many as those who have been infected. The fact that new outbreaks have been seen in Texas and Arizona suggests that this shortfall in testing likely disguised early outbreaks that could have been detected otherwise.


```{r tpi,fig.asp=.8,fig.cap="Measuring States' Testing Rates Relative to Infection Rates",echo=F,message=F,warning=F}

us_fit_scale <- readRDS("../data/us_fit_scale.rds")
combined <- readRDS("combined.rds")
real_data <- readRDS("real_data.rds")
combined <- mutate(ungroup(combined),key=1:n()) %>% 
  group_by(state) %>% 
  arrange(state,month_day) %>% 
  mutate(cum_sum_cases=cases,
         recovered=coalesce(recovered,0),
         deaths=coalesce(deaths,0))
         # cum_sum_cases = cum_sum_cas

prop_infected_mat <- t(as.matrix(us_fit_scale,"prop_infect_out"))

prop_infected <- as_tibble(prop_infected_mat) %>% 
  mutate(key=1:n()) %>% 
  gather(key="iter",value="estimate",-key)

all_est_state <- select(combined,deaths,recovered,month_day,state,key,cum_sum_cases,state_pop,Difference,tests3,test_max) %>% 
  left_join(prop_infected,by="key")

# merge in deaths/recovered

us_case_count <- group_by(combined,month_day) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases),
            all_rec=sum(recovered),
            all_death=sum(deaths))

all_est_state <- left_join(all_est_state,us_case_count,by="month_day")

# convert to marginal changes

this_infect1 <- lapply(seq(max(combined$month_day) - days(7), max(combined$month_day),by=1),
                    function(d) {
                      
                      filter(ungroup(all_est_state),
                       month_day==d) %>% 
  select(state,estimate,iter) %>% 
  group_by(state) %>% 
  mutate(estimate=plogis(estimate)) %>% 
  spread(key="state",value="estimate") %>% 
  ungroup %>% 
  select(-iter) %>% 
  as.matrix
       })

this_test_max <- ungroup(combined) %>% 
  select(state,month_day,test_max) %>% 
  filter(month_day==max(month_day))

this_time <- max(combined$lin_counter)

test_lin <- as.matrix(us_fit_scale,"test_lin_counter")
test_lin2 <- as.matrix(us_fit_scale,"test_lin_counter2")
test_base <- as.matrix(us_fit_scale,"test_baseline")

alpha_test <- as.data.frame(us_fit_scale,pars="alpha_test")

test_max_par <- as.data.frame(us_fit_scale,"test_max_par")

lin_step <- real_data$lin_counter[2,1] - real_data$lin_counter[1,1]

country_test <- as.matrix(us_fit_scale,"country_test_raw")
mu_test <- as.matrix(us_fit_scale,"mu_test_raw")
sigma_test <- as.matrix(us_fit_scale,"sigma_test_raw")

country_nonc <- mu_test[,1] + country_test * sigma_test[,1]

# loop over infections
# loop over time

over_days <- lapply(seq(max(combined$month_day) - days(7), max(combined$month_day),by=1),
                    function(d) {
                      
    over_states <- lapply(1:real_data$num_country, function(s) {
      
      lin_val <- unique(combined$lin_counter[combined$month_day==d])
      counter <- which(seq(max(combined$month_day) - days(7), max(combined$month_day),by=1)==d)
      
     tibble(estimate=plogis(alpha_test$alpha_test + country_nonc[,s] * this_infect1[[counter]][,s]*lin_val +
                            test_base * this_infect1[[counter]][,s] + test_lin*lin_val),
         state_num=s,
         mean_est=mean(this_infect1[[counter]][,s]),
         month_day=d)
    }) %>% bind_rows
                      
}) %>% bind_rows




saveRDS(over_days,"test_data.rds")

state_id <- distinct(combined,state,state_pop) %>% 
  ungroup %>% 
  mutate(state_num=as.numeric(factor(state)))

over_days %>%
  left_join(state_id,by="state_num") %>%
  group_by(state) %>%
    summarize(med_est=quantile(estimate/mean_est,.5),
            high_est=quantile(estimate/mean_est,.95),
            low_est=quantile(estimate/mean_est,.05)) %>% 
  ggplot(aes(y=med_est,x=reorder(state,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est)) +
  theme_minimal() +
  theme(panel.grid = element_blank(),axis.text=element_text(size = 7)) +
  coord_flip() +
  geom_hline(yintercept=.01) +
  xlab("") +
  labs(caption = stringr::str_wrap("Figure shows the average number of additional people tested in a given state for each person who becomes infected. Estimate is a cumulative average of the last seven days of data.")) +
  ylab("Total Tested for Each Infected Resident (Cumulative Average)")

ggsave("testing.png",height=5,width=8)

```

We would note that this information is also helpful to policy makers and others trying to make sense of observed case counts given the limitation in testing thus far. Our estimates help take into account these known biases and adjust them based on differences between states and within states in terms of disease trajectories. We believe this model can be used to help understand disease trends and factors associated with it even in the relatively data-poor environment many countries find themselves in. 

# Simulation

Because our model is fully generative, we can simulate it using Monte Carlo methods. The simulation presented here is a simplification of the model presented in the main paper for clarity of exposition. We do not have varying polynomial time trends but rather a single global time trend, and we do not implement the cumulative sum transformation on the latent vector. 
Despite these simplifications, the simulation is very important as it is the only way to demonstrate that the model is globally identified and can in fact capture unobserved parameters like suppression effects and relative infection rates. We also are able to show how the bias in only using observed cases and tests can affect the estimates of parameters.

The following R code generates data from the model and plots the resulted unobserved infection rate and observed values for tests and cases along with an exogenous suppression covariate:

```{r onset,fig.width=7,fig.height=6,fig.cap="Simulation of Observed Tests and Cases Given Unobserved Infectious Process"}

# simulation parameters
num_state <- 50

time_points <- 100
# allows for linear growth that later becomes explosive
polynomials <- c(.03,0.0003,-0.00001)

# factor that determines how many people a state is willing/able to test
# states that suppress or don't suppress
# induce correlation between the two

 cor_vars <- MASS::mvrnorm(n = num_state, mu = c(0, 5),
                              Sigma = matrix(c(1, -.5, -.5, 1), 2, 2))


state_test <- cor_vars[, 2]
suppress_measures <- cor_vars[, 1]

# size of states

state_pop <- rpois(num_state, 10000)

# assumt t=1 is unmodeled = exogenous start of the infection
  
t1 <- c(1, rep(0, num_state-1))

# create a suppression coefficient
# first is for preventing domestic transmission from occuring
# second is for preventing further domestic transmission once it starts

suppress1 <- -0.5
suppress2 <- -0.05

# high value of phi = high over-time stability
phi <- c(300, 300)

# parameter governing how hard it is to find infected people and test them
# strictly positive

finding <- 1.5

# recursive function to generate time-series data by state

out_poly <- function(time_pt, end_pt, time_counts, tested, case_count, rate_infected, pr_domestic) {
  
  if(time_pt==1) {
    time_counts <- as.matrix(c(1, rep(0, num_state-1)))
    rate_infected <- as.matrix(c(.0001, rep(0, num_state-1)))
    tested <- as.matrix(rep(0, num_state))
    case_count <- as.matrix(c(1, rep(0,num_state-1)))
  }
  
  # if at time = t infected, start time tracker at t
  # need to know how many states have reported at least one case = infection start
  
  world_count <- sum(case_count[, time_pt]>0)
  
  
  if(time_pt==1) {
    
    rate_infected_new <-  plogis(-5 + time_counts[, time_pt]*polynomials[1] + 
                                   suppress1*suppress_measures +
                    suppress2*suppress_measures*time_counts[, time_pt] +
                    .05*sum(world_count) +
      (time_counts[, time_pt]^2)*polynomials[2] + 
        (time_counts[, time_pt]^3)*polynomials[3])
    
    # conservative time counter that only starts when first case is recorded
    
    time_counts_new <- ifelse(time_counts[, time_pt]>0 | case_count[, time_pt]>0, time_counts[, time_pt]+1, time_counts[, time_pt])
    
  } else {
    
    rate_infected_new <- plogis(-5 + time_counts[, time_pt]*polynomials[1] + 
                    suppress1*suppress_measures +
                    suppress2*suppress_measures*time_counts[, time_pt] +
                    .05*sum(world_count) +
      (time_counts[, time_pt]^2)*polynomials[2] + 
        (time_counts[, time_pt]^3)*polynomials[3])
    
    # conservative time counter that only starts when first case is recorded
    
    time_counts_new <- ifelse(time_counts[, time_pt]>0 | case_count[, time_pt]>0, time_counts[, time_pt]+1 ,time_counts[, time_pt])
  }

  # of these, need to calculated a set number tested
  mu_test <- plogis(-7 + state_test*rate_infected_new)
  tested_new <- rbbinom(num_state, state_pop, mu_test*phi[1], (1-mu_test)*phi[1])
  
  # determine case count as percentage number tested
  # this is what we always observe
  mu_case <- plogis(-2.19 + finding*rate_infected_new)
  case_count_new <- rbbinom(num_state, tested_new, mu_case*phi[2], (1-mu_case)*phi[2])
  
  
  if(time_pt<end_pt) {
    out_poly(time_pt = time_pt+1,
             end_pt = end_pt,
             time_counts = cbind(time_counts,
                               time_counts_new),
             rate_infected = cbind(rate_infected, rate_infected_new),
             tested = cbind(tested, tested_new),
             case_count = cbind(case_count, case_count_new))
  } else {
    return(list(time_counts = time_counts,
                tested = tested,
                rate_infected = rate_infected,
                case_count = case_count))
  }
  
}

check1 <- out_poly(1, time_points)

check1 <- lapply(check1, function(c) {
  colnames(c) <- as.numeric(1:time_points)
  c
})

all_out <- bind_rows(list(time_counts=as_tibble(check1$time_counts),
                          `Proportion Population\nInfected`=as_tibble(check1$rate_infected),
                          `Number of Cases`=as_tibble(check1$case_count),
                          `Proportion of Cases from Domestic Transmission`=as_tibble(check1$pr_domestic),
                          `Number of Tests`=as_tibble(check1$tested)),.id="Series")

all_out$state <- rep(paste0("state_",1:num_state),times=length(check1))
all_out$suppress_measures <- rep(suppress_measures,times=length(check1))

all_out %>% 
  gather(key = "time_id",value="indicator",-Series,-state,-suppress_measures) %>% 
  mutate(time_id=as.numeric(time_id)) %>% 
  filter(!(Series %in% c("time_counts"))) %>% 
  ggplot(aes(y=indicator,x=time_id)) +
  geom_line(aes(colour=suppress_measures,group=state),alpha=0.3) +
  xlab("Days Since Outbreak") +
  ylab("") +
  facet_wrap(~Series,scales="free_y") +
  theme(panel.background = element_blank(),
        panel.grid=element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(face="bold"),
        legend.position = "top")
```


Figure \@ref(fig:onset) shows one line for each state's trajectory from a total of 100 states and 100 time points.
As can be seen, the shading indicating strength of suppression policies diverges substantially over time.
However, the numbers of observed tests and cases show far more random noise due to the difficulty in inferring the true rate from the observed data.
It is possible that some states simply want to test more, and end up with more cases, or that the infection rate is in fact higher. As such, this model is able to incorporate that measurement uncertainty between the true (unobserved) rate and the observed indicators, tests and cases.

The data were also generated so that the suppression covariate, which shades the infection rates in Figure \@ref(fig:onset), is positively correlated with a state's willingness to test.
As such, this covariation will lead a naive model to dramatically *over-estimate* the effect of suppression policies as the case/test ratio mechanically falls due to increased tests independent of the infection rate. 

The primary advantage of this model is that it allows for the testing of covariates that affect the true infection rate without requiring more heavy-duty approaches like SEIR/SIR, such as modeling reproduction numbers and other disease-specific mechanisms.
The intention is to have a more parsimonious model that can see how the effect of variables like suppression measures have on different states/states infection numbers over time.
In particular, this model increases our understanding of the connection between contextual factors and human behavior to the virus' progression.

The model could be further extended with more complicated processes, such as spatial modeling, but for the purposes of this exposition we do not look further at such extensions. We would note that the world infection parameter is implicitly, if not explicitly, a spatial measure. 

## Estimation

We can then fit an empirical model using the Hamiltonian Monte Carlo (HMC) Markov Chain Monte Carlo (MCMC) sampler in Stan [@carpenter2017]
to model the unobserved infection rate given the simulated observed data.
We also fit a Bayesian binomial model of the proportion of counts to state population using the same specification as \@ref(eq:binom).
This naive model is fitted to indicate the amount of bias that can occur in estimates as a result of ignoring the underlying infection process.

```{r stan_code}

# all data from simulation
# primarily case and test counts

# need to make centered, ortho-normal polynomials

ortho_time <- poly(scale(1:time_points, scale = F),degree=3)

init_vals <- function() {
  list(phi1 = 300,
       phi2 = 300,
       world_infect = .1,
       finding = 1,
       poly = c(0, 0, 0),
       state_test = rnorm(num_state, 5, .25),
       alpha = c(-7, 0,-2),
       sigma_test_raw = 1)
}

sim_data <- list(time_all = time_points,
                 num_country = num_state,
                 country_pop = state_pop,
                 cases = check1$case_count,
                 S = 1,
                 phi_scale = 1/100,
                 ortho_time = ortho_time,
                 tests = check1$tested,
                 count_outbreak = as.numeric(scale(apply(check1$time_counts, 2, function(c) sum(c>0)),
                                                 scale = FALSE)),
                 time_outbreak = check1$time_counts,
                 time_outbreak_center = matrix(scale(c(check1$time_counts), scale = FALSE), nrow = nrow(check1$time_counts),
                                                   ncol = ncol(check1$time_counts)),
                 suppress = as.matrix(suppress_measures))

# need to make a regular type data frame to do observed modeling with rstanarm

obs_data <- as_tibble(check1$case_count) %>% 
  mutate(num_state = 1:n()) %>% 
  gather(key = "time_points", value="cases", -num_state)
  

# join in outbreak timing + covariates

time_data <- as_tibble(sim_data$time_outbreak) %>% 
  mutate(num_state = 1:n()) %>% 
  gather(key = "time_points", value = "time_outbreak", -num_state) %>% 
  mutate(time_points = stringr::str_extract(time_points, "[0-9]+"))

obs_data <- left_join(obs_data, time_data, by = c("time_points", "num_state")) %>% 
  left_join(tibble(state_pop=state_pop,
                   suppress=suppress_measures,
                   num_state=1:length(state_pop)),by="num_state") %>% 
  mutate(time_points=as.numeric(time_points),
         time_points_scale=as.numeric(scale(time_points,scale=T))) %>% 
  left_join(tibble(count_outbreak=sim_data$count_outbreak,
                   time_points=as.numeric(1:time_points)),by="time_points")

if(run_model) {
  
  pan_model <- stan_model("corona_tscs_betab.stan")

# run model

pan_model_est <- sampling(pan_model,data=sim_data,chains=2,cores=2,iter=1200,warmup=800,init=init_vals,
                          control=list(adapt_delta=0.95))

naive_model <- stan_glm(cbind(cases,state_pop-cases)~poly(time_points_scale,3) +
                          count_outbreak +
                          suppress + 
                          suppress:poly(time_points_scale,3)[,1],
                        data=obs_data,
                        family="binomial",
                        cores=1,
                        chains=1)

saveRDS(pan_model_est,"../data/pan_model_est.rds")
saveRDS(naive_model,"../data/naive_model_sim.rds")
} else {
  pan_model_est <- readRDS("../data/pan_model_est.rds")
  naive_model <- readRDS("../data/naive_model_sim.rds")
}

```

After fitting the latent model, we can access the estimated infection rates and plot them:

```{r plotdata,fig.cap="Estimated Simulated Infection Rates"}

all_est <- as.data.frame(pan_model_est,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(estimate=estimate) %>% 
  summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")))

all_est <- left_join(all_est,tibble(state_num=1:num_state,
                                    suppress_measures=suppress_measures),by="state_num")

all_est %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est,
  group=state_num,
  fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_brewer(type="div") +
  ylab("Latent Infection Scale") +
  xlab("Days Since Outbreak Start") +
  theme(panel.grid = element_blank(),
        legend.position = "top") 

```


We see how the model is able to partially recover the infection rate as shown in Figure \@ref(fig:plotdata).
The estimates reveal the same general arc as the generated data, with higher suppression policies associated with lower infection counts, but the scale of the infection rate is no longer identified.
As such, the model is only able to recover the relative rather than absolute trajectory of the infection rate. 

However, because we have inferred the correct arc, we can also see what the estimated suppression parameters are.
We also compare those to the same suppression parameters from the naive model in Figure \@ref(fig:suppeff).

```{r suppeff,fig.cap="Recovered Simulated Virus Suppression Parameters Versus Naive Model"}

require(bayesplot)
require(patchwork)

p1 <- mcmc_intervals_data(as.array(pan_model_est,pars=c("suppress_effect[1,1]",
                                                "suppress_effect[2,1]")))

p2 <- mcmc_intervals_data(naive_model,pars=c("suppress",
                                                "suppress:poly(time_points_scale, 3)[, 1]"))

bind_rows(list(Latent=p1,
               Observed=p2),.id="Model") %>% 
  mutate(parameter=recode(parameter,
                          `suppress_effect[1,1]`="Constant\nEffect",
                          `suppress_effect[2,1]`="Over-Time\nEffect",
                          `suppress`="Constant\nEffect",
                          `suppress:poly(time_points_scale, 3)[, 1]`="Over-Time\nEffect")) %>% 
  ggplot(aes(y=m,x=Model)) +
  geom_pointrange(aes(ymin=ll,ymax=hh),position=position_dodge(0.5)) +
  theme_minimal() +
  geom_hline(yintercept=0,linetype=3) +
  scale_color_brewer(type="qual") +
  guides(color="none") +
  ylab("Parameter Estimate") +
  theme(panel.grid = element_blank(),
        legend.position = "top") +
  coord_flip() +
  facet_wrap(~parameter,scales="free_x",ncol=1)

```


We can see in Figure \@ref(fig:suppeff) that despite the uncertainty in not perfectly observing the infection rate, we can still get a precise credible interval on the suppression effect.
However, while the observed model's parameters have the same sign, they are wildly inflated, and far too precise.
The constant effect in particular is ten times the size of the latent model with virtually no uncertainty interval.
Because the infection process is ignored, the model obfuscates the infection/testing relationship with the effect of suppression policies, wildly over-estimating their effect at lowering infection counts.

The advantage of this model, as can be seen, is that with testing numbers and case counts, we can model the effect of state-level (or region-level) variables on the unseen infection rate up to an unknown constant.
It is far simpler than existing approaches while still permitting inference on these hard-to-quantify measures.
It is no substitute for infectious disease modeling--for example, it produces no estimates of the susceptible versus recovered individuals in the population, nor death rates--but rather a way to measure the effect of different background factors of interest to social and natural sciences on disease outcomes as well as approximate disease trajectory. Furthermore, the model's main quantity is a better measure to share with the public than misleading numbers of positive case counts.

While the partial identification of this model is interesting and potentially useful in situations where nothing else can be inferred about the virus, it is possible to further identify the scale of the latent variable by incorporating information from SIR/SEIR models, which we demonstrate in the next section.

# Identifying the Latent Scale

To show how we can further attempt to identify the scale of the latent variable, instead of only relative differences between states, we show in this section how we can add in information from SEIR/SIR modeling to identify the total number of infected persons in the model.
The crucial missing piece of information in the model is the ratio between the proportion of infected persons $I_{ct}$ and the proportion of tests per state population $q_{ct}$:

\begin{equation}
\frac{q_{ct}}{I_{ct}}
\end{equation}

Another way of framing the problem is to think of how much the number of tests should increase given a one-percentage increase in the infection rate.
How many of these infected people will be tested? Without an idea of the true number of infected people, it is impossible to answer this question and thus identify the latent scale.

However, an increasing number of SIR/SEIR papers show that it is likely that as few as 10% of the total number of infected persons are actually recorded as cases, including in the United States.
This number provides us with a conservative lower bound if we consider that the number of tests should be at least 10% of the total proportion of infected persons.
In other words, we can consider adding the following information into the model:

\begin{equation}
\frac{q_{ct}}{I_{ct}} >0.1.
\end{equation}

Every percentage increase in the infection rate should result in at least a 0.1% increase in the total number of people tested as a percentage of the population, though the upper bound is unknown (some states may test many more than are actually infected).
It is difficult to impose this constraint directly in the model; however, we can consider adding it as prior information if we can define a prior density over the ratio.
First, for computational simplicity, we can consider a distribution over the log differences of the parameters:

\begin{equation}
\text{log} q_{ct} - \text{log} I_{ct}.
\end{equation}

By simulating from uniform distributions of possible rates for $\text{log} q_{ct}$ and $\text{log} I_{ct}$, it is possible to tell that the a realistic distribution of log differences with small density less than 0.1 is in fact very close a standard normal distribution:

```{r genlogs,fig.cap="Distribution of Log Difference in Probabilities"}
# this code generates a log distribution of a ratio of two probabilities.
f_y <- function(y, a, b, log = FALSE){
  lc <-  log(b-a)
  ans <- y  - lc
  if(!log) ans <- exp(ans)
  return(ans)
}
#
get_Y_var <- function(a, b){
  EXsq <- ( b*(log(b)^2 - 2*log(b) + 2) -  a*(log(a)^2 - 2*log(a) + 2) )/(b-a)
  EX <- (b*(log(b) - 1) - a*(log(a) - 1) ) /(b-a)
  ans <- EXsq - (EX)^2
  return(ans)
}
##
analytic_W <- function(w, a, b){  ## assumes i.i.d.
  c0 <- all(w > a/b, w < b/a)
  k <- (b-a)*(b-a)
  m <- max(a, a/w)
  M <- min(b, b/w)
  soln <- function(L, U) ((U *abs(U))- (L *abs(L)))/(2*k)
  d0 <- soln(L = m, U = M)
  dens <- c0 * d0 
  return(dens)  
}
analytic_W <- Vectorize(analytic_W)
##
f_z <- function(z, a, b, log = FALSE){
  ans <- log(analytic_W(exp(z), a, b)) + z
  if(!log) ans <- exp(ans)
  return(ans)
}
f_z <- Vectorize(f_z)
###############

M <- 1E6
a <-  .01
b <-  .5
Y <- log(runif(M, min = a, max = b))

#hist(Y, probability = TRUE)
# curve(f_y(x, a, b), min(Y), max(Y), add = TRUE, lwd = 2)
# abline(v = c(log(a), log(b)), lwd = 2, lty = 2)
# integrate(function(x) f_y(x, a, b), log(a), log(b))

###

Y1 <- log(runif(M, min = a, max = b))
Y2 <- log(runif(M, min = a, max = b))

`Log Difference` <- Y1 - Y2

# integrate(function(x) f_z(x, a, b), log(a)-log(b), log(b)-log(a))
# integrate(function(x) x^2* f_z(x, a, b), log(a)-log(b), log(b)-log(a))
# 
# 2*get_Y_var(a, b)
# Var(`Log Difference`)
# 
hist(`Log Difference`, breaks = 50, probability = TRUE)
curve(f_z(x, a, b), min(`Log Difference`), max(`Log Difference`), col = 2, add = TRUE)
curve(dnorm(x, 0, sqrt(2*get_Y_var(a, b))), min(`Log Difference`), max(`Log Difference`), col = 3, add = TRUE)
abline(v = c(log(a)-log(b), log(b)-log(a)), lwd = 2, lty = 2)
```

If one desires to choose different parameters $a$ and $b > a$ for the distribution of $\text{log} q_{ct}$ and $\text{log} I_{ct}$, one may either (i) use the code above (`get_Y_var`) to obtain an approximating Gaussian -- with variance given by `2*get_Y_var(a, b)` -- or (ii) use the exact expression for the log of the ratio of two i.i.d. uniform random variables (`f_z`).
Here, we will employ the standard Gaussian as it is good and tractable approximation of the exact distribution.

We can see in the figure above that the standard normal provides an approximate fit to the density of two probabilities where there is limited density on values below $\text{log } -2$ or 0.14.
On the other hand, the prior puts substantial mass on ratios of tests to infected that are quite high, such as $\text{log} 2$, 7.38. This informative prior, which still allows for a range of possible ratios, allows me to use SEIR/SIR model conclusions while still permitting uncertainty over the underlying relationship.

For these reasons, we add this term to the joint posterior for each time point $t$ and state $c$:

\begin{equation}
\text{log} q_{ct} - \text{log} I_{ct} \sim \operatorname{Normal}(0, 1).
\end{equation}

We also add a log Jacobian adjustment to the posterior density to reflect the fact that this prior is a non-linear function of parameters we have already assigned priors to:
\begin{equation}
\text{log} (1 - q_{ct}) + \text{log} (1 - I_{ct}).
\end{equation}

The Jacobian adjustment is the derivative of the non-linear functions with respect to the underlying parameters, which in this case are the natural log and the inverse logit function.

# References
