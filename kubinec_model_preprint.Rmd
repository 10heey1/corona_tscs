---
title: "A Retrospective Bayesian Model for Measuring Policy Impact on Observed Disease Test and Case Counts"
author: "Robert Kubinec"
date: "March 30th, 2020"
output: 
  bookdown::pdf_document2:
    includes: 
      in_header:
          preamble.tex
bibliography: BibTexDatabase.bib
abstract: "In this paper I put forward a Bayesian model of observed COVID-19 cases and test counts. I show that this observed data cannot be used to project or measure the true infection rate due to confounding between the unobserved number of infected and the observed number of tests and cases. Instead, I show that it is possible to identify the rank and sign of the effect of tertiary factors factors on the count of infected up to the present time point. This retrospective model is useful for researchers in the  social sciences who are collecting increasing amounts of data about political, economic and social factors related to the disease's spread but who have no need to model the disease itself. To apply the model, I show that as of March 30th, U.S. states with.^[To reproduce the model and to access the underlying Stan code, please see my [Github page](https://github.com/saudiwin/corona_tscs). This paper is part of the [CoronaNet project](https://lumesserschmidt.github.io/CoronaNet/) collecting data on government responses to the COVID-19 pandemic. For helpful comments I thank Cindy Cheng and Joan Barcelo.]"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE)
require(dplyr)
require(tidyr)
require(ggplot2)
require(rstan)
require(stringr)
require(lubridate)
require(bayesplot)
require(historydata)
require(readr)
require(datasets)
require(extraDistr)

set.seed(662817)

rstan_options(auto_write=T)

knitr::opts_chunk$set(warning=F,message=F)

# whether to run model (it will take a few hours) or load saved model from disk

run_model <- F

```


# Introduction

In recent days as more and more data is available on observed case counts of the SARS-CoV2 coronavirus, there are also increasing attempts to infer infection rates from these case counts, including by social science researchers. The temptation to make inferences from the observed data, however, can lead to misleading conclusions. For example, some policy makers like Deborah Bix have begun to question whether the predictions of epidemiological models are far worse than the observed data.^[See article available at https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d] By contrast, in this paper I show that it is impossible to infer the true number of infected people from observed tests and cases alone due to the infection rate confounding both observed indicators. As a result, the susceptible-infected-recovered (SIR)/susceptible-exposed-infected-recovered (SEIR) approach  [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020] is shown to be the only approach that can provide bounds on the the true number of infected given reasonable assumptions. 

However, this observation still leaves the door open to what can be learned from observed data, and whether the available data are useful for research outside of epidemiology, such as the growing work into the implications of economic, political and social factors on for the disease. The intention of this paper is to offer a retrospective Bayesian model of the empirical data, the observed tests and cases, that allows researchers to test for the effect of suppression policies and other region-level factors. I show in this paper that although we cannot infer the total number of infected from the observed data, we can learn from comparing relative growth rates across regions by measuring the time trend of the rate of infection. By doing so we can rank- and sign-identify the effect of policies given minimal assumptions even without knowing the true infection rate. Furthermore, by using informed priors from SIR/SEIR models, I show how estimates of this partially-identified model can be transformed to reasonable estimates of the retrospective number of infected persons. While the model cannot predict disease progression nor inform medical studies of the disease, its primary aim is to help researchers in social and behavioral fields studying tertiary factors on the spread of SARS-CoV2.

In this paper, I show how the model can be applied by measuring the association between U.S. state-level factors and the disease as of March 30th, 2020, including the timing of state of emergency declarations, vote share in the 2016 election for President Donald Trump, and relevant control variables including gross domestic product (GDP) and hospital beds per capita. 

In this paper I first present a limited overview of the model before applying it to the empirical data of test and case counts in the U.S. states.

# Model Overview

The compartmental model employed by epidemiologists to study disease, and in particular SARS-CoV2 [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], estimates different classes of individuals in the population, denoted $S$ for susceptible, $I$ for infected, and $R$ for recovered (other letters may be added such as $E$ for exposed). Using ordinary differential equations, the count of the infected can then be estimated by equating these three identities assuming a fixed population size, as seems reasonable during a relatively quick epidemic. These models guide our understanding of the disease and its progression, and have made warnings about the disease's spread that are proving true on a daily basis.

By contrast, this paper endeavors to estimate a much simpler quantity than the disease progression. For a number of time points $t \in T$ since the outbreak's start and countries/regions $c \in C$,, I aim to identify the following quantity:

\begin{equation}
f_t \left (\frac{I_{ct}}{S_{ct}+R_{ct}} \right )
\end{equation}

Assuming a fixed population size, this quantity is simply the rate of infections in the population up to the present. Because these are the quantities themselves, not their derivatives, there is no attempt to simultaneously solve for all the parameters. The function $f_t$ determines the historical time trend of the rate of infection (which is assumed to be same across countries/regions) in the population up to time $t$, the present. Because the denominator is shifting over time due to disease progression dynamics, this model is only useful for retrospection, i.e., to examine factors that may be influencing the known and measurable time trend $f_t$ up to the present. As $S_{ct}$ and $R_{ct}$ are exogenous to the model, the model cannot predict future prevalence of the disease given that it does not determine these crucial factors. 

I reserve the full exposition of the model for the supplementary materials. In essence I backwards infer the infection rate $I_{ct}$ as a latent process given observed test and counts. However, this model cannot be identified without further restrictions and informative priors. The reason for this is shown in Figure \ref{tikzfig}. Increasing infection rates can cause both increasing numbers of observed counts $a_{ct}$ and tests $q_{ct}$. As more people are infected, more tests are likely to be done, which will increase the number of cases independently. As a result, due to the back-door path from the infection rate $I_{ct}$ to the number of tests $q_{ct}$, it is impossible to infer from the observed data alone. 

\begin{figure}
\label{tikzfig}
\caption{Directed Acyclic Graph Showing Confounding of Covariate $X_{ct}$ on Observed Tests $q_{ct}$ and Cases $a_{ct}$ Due to Unobserved Infection Rate $I_{ct}$}
\ctikzfig{policy_dag}
\footnotesize{Figure shows the relationship between a covariate $X_{ct}$ representing a policy or social factor influencing the infection rate $I_{ct}$. Because the infection rate $I_{ct}$ incluences both the number of reported tests $q_{ct}$ and reported cases $a_{ct}$, any regression of a covariate $X_{ct}$ on the reported data will be biased.}
\end{figure}

I show in the supplementary materials that two restrictions are sufficient to identify the sign and rank of the effect of covariates $X_{ct}$ on $I_{ct}$: the paths $I_{ct} \rightarrow q_{ct}$ and $I_{ct} \rightarrow a_{ct}$ must be strictly positive, while the path $X_{ct} \rightarrow I_{ct}$ must be zero or negative. Given these restrictions and weakly informative priors on the parameters, it is possible to know whether $X_{ct}$ is associated with increasing or decreasing $I_{ct}$, though not with respect to the actual number, only its sign or rank relative to other covariates.

Furthermore, I show in the supplementary materials that if further bounds can be put on the true number of infected individuals $I_{ct}$, it is possible to transform inferences from the model to actual counts of infected people in the present. By doing so, it is possible to come up with an empirical estimate of infection that is more useful to the general public than solely observed tests and cases, as I demonstrate in the next section.


# Estimation with Data

In this section I fit the model to numbers of COVID-19 case counts on US states and territories provided by [The New York Times](https://github.com/nytimes/covid-19-data). I supplement these observed case counts with testing data by day from the [COVID-19 Tracking Project](https://github.com/COVID19Tracking/covid-tracking-data). The testing data starts at March 4th, so we will impute the testing data back in time by assuming that the average case/tests ratio stays the same to the origin of the outbreak. Furthermore, as there are discrepancies where the reported number of tests in some states like New Jersey is less than the total number of cases, I impute the number of tests via the case/test ratio for the sample as a whole.

To analyze the effect of suppression policies, I proxy for preparedness to fight the epidemic by including the date that states of emergencies were declared across U.S. states and territories in the model.^[See this site for dates and relevant sources: https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_2020_coronavirus_pandemic]. The suppression covariate is then equal to a state X 1 vector of days which is mean centered and standardized. I further add in state-level data on Donald Trump's vote share for the 2016 election, a 2019 estimate of state GDP from the Bureau of Economic Analysis, and a ranking of air pollution, cardiovascular deaths per capita, dedicated health care providers, preventable hospitalizations, and smoking rates provided by the United Health Foundation.

In this section I first fit a partially-identified model without any information about the true number of infected people to demonstrate that we can identify the sign and relative rank effect of suppression policies without this information. I then show how we can use insight from SIR/SEIR models to provide bounds on the total number of infected people. 



```{r munge_data,include=F}

data(us_state_populations)

state_pop <- filter(us_state_populations,year==2010) %>% 
  select(state,population)

merge_names <- tibble(state.abb,
                      state=state.name)

nyt_data <- read_csv("~/covid-19-data/us-states.csv") %>% 
  complete(date,state,fill=list(cases=0,deaths=0,fips=0)) %>% 
  mutate(month_day=ymd(date)) %>% 
  group_by(state) %>% 
    arrange(state,date) %>% 
  mutate(Difference=cases - dplyr::lag(cases),
         Difference=coalesce(Difference,0,0)) %>% 
  left_join(merge_names,by="state")

tests <- read_csv("~/covid-tracking-data/data/states_daily_4pm_et.csv") %>% 
  mutate(month_day=ymd(date)) %>% 
  arrange(state,month_day) %>% 
  group_by(state) %>% 
  mutate(tests_diff=total-dplyr::lag(total),
         cases_diff=positive-dplyr::lag(positive),
         cases_diff=coalesce(cases_diff,positive),
         cases_diff=ifelse(cases_diff<0,0,cases_diff),
         tests_diff=coalesce(tests_diff,total),
         tests_diff=ifelse(tests_diff<0,0,tests_diff)) %>% 
  select(month_day,tests="tests_diff",total,state.abb="state")

# merge cases and tests

combined <- left_join(nyt_data,tests,by=c("state.abb","month_day")) %>% 
  left_join(state_pop,by="state") %>% 
  filter(!is.na(population))

# add suppression data

emergency <- read_csv("data/state_emergency_wikipedia.csv") %>% 
  mutate(day_emergency=dmy(paste0(`State of emergency declared`,"-2020")),
         mean_day=mean(as.numeric(day_emergency),na.rm=T),
         sd_day=sd(as.numeric(day_emergency),na.rm=T),
         day_emergency=((as.numeric(day_emergency) - mean_day)/sd_day)) %>% 
  select(state="State/territory",day_emergency,mean_day,sd_day) %>% 
  mutate(state=substr(state,2,nchar(state))) %>% 
  filter(!is.na(day_emergency))

combined <- left_join(combined,emergency,by="state")

# impute data

combined <- group_by(combined,state) %>% 
  mutate(test_case_ratio=sum(tests,na.rm=T)/sum(Difference,na.rm=T)) %>% 
  ungroup %>% 
  mutate(test_case_ratio=ifelse(test_case_ratio<1 | is.na(test_case_ratio),
                                mean(test_case_ratio[test_case_ratio>1],na.rm=T),test_case_ratio)) %>% 
  group_by(state) %>% 
    mutate(tests=case_when(Difference>0 & is.na(tests)~Difference*test_case_ratio,
                    Difference==0~0,
                    Difference>tests~Difference*test_case_ratio,
                    TRUE~tests)) %>% 
  arrange(state)

# create case dataset

cases_matrix <- select(combined,Difference,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(Difference=as.integer(mean(Difference))) %>% 
  spread(key = "month_day",value="Difference")

cases_matrix_num <- as.matrix(select(cases_matrix,-state))

# create tests dataset

tests_matrix <- select(combined,tests,month_day,state) %>% 
  group_by(month_day,state) %>% 
  summarize(tests=as.integer(mean(tests))) %>% 
  spread(key = "month_day",value="tests")

tests_matrix_num <- as.matrix(select(tests_matrix,-state))

# need the outbreak matrix

outbreak_matrix <- as.matrix(lapply(1:ncol(cases_matrix_num), function(c) {
  if(c==1) {
    outbreak <- as.numeric(cases_matrix_num[,c]>0)
  } else {
    outbreak <- as.numeric(apply(cases_matrix_num[,1:c],1,function(col) any(col>0)))
  }
  tibble(outbreak)
}) %>% bind_cols)

colnames(outbreak_matrix) <- colnames(cases_matrix_num)

time_outbreak_matrix <- t(apply(outbreak_matrix,1,cumsum))

just_data <- distinct(combined,state,day_emergency,population) %>% arrange(state)

# now give to Stan

ortho_time <- poly(scale(1:ncol(cases_matrix_num)),degree=3)

real_data <- list(time_all=ncol(cases_matrix_num),
                 num_country=nrow(cases_matrix_num),
                 country_pop=floor(just_data$population/100),
                 cases=cases_matrix_num,
                 ortho_time=ortho_time,
                 phi_scale=.1,
                 count_outbreak=as.numeric(scale(apply(outbreak_matrix,2,sum))),
                 tests=tests_matrix_num,
                 time_outbreak=time_outbreak_matrix,
                 suppress=just_data$day_emergency)

init_vals <- function() {
  list(phi_raw=c(10,10),
       world_infect=0,
       finding=1,
       alpha=c(0,0))
}


if(run_model) {
  us_fit <- sampling(pan_model,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,control=list(adapt_delta=0.95),
                   init=init_vals)
  
  saveRDS(us_fit,"data/us_fit.rds")
} else {
  us_fit <- readRDS("data/us_fit.rds")
}


```

```{r infect_by_state}
all_est_state <- as.data.frame(us_fit,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")

all_est_state %>% 
  mutate(estimate=estimate) %>% 
  group_by(state_num,time_point,suppress_measures) %>% 
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_line(aes(group=state_num,colour=suppress_measures),alpha=0.5) +
  stat_smooth() +
  theme_minimal() +
  scale_color_distiller(palette="RdBu",direction=-1) +
  ylab("Latent Infection Scale") +
  ggtitle("Measuring Relative Infection Rates by U.S. State with Total Average",
          subtitle="Lines Colored by When State Declared State of Emergency") +
  labs(caption="As the total number of infected people is unknown, this chart measures the relative\ninfection rates between states.") +
  xlab("Days Since Outbreak Start") +
  geom_hline(yintercept = 0,linetype=3) +
  guides(color=guide_colorbar(title="Timing of State Emergency Declaration")) +
  theme(panel.grid = element_blank(),
        legend.position = "top") 


all_est_state %>% 
  mutate(estimate=estimate) %>% 
  group_by(state_num,time_point,suppress_measures) %>% 
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  #geom_line(aes(group=state_num,colour=suppress_measures),alpha=0.5) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est,
  group=state_num,
  fill=suppress_measures),alpha=0.5) +
  stat_smooth() +
  theme_minimal() +
  scale_color_distiller(palette="RdBu",direction=-1) +
  ylab("Latent Infection Scale") +
  ggtitle("Uncertainty in Relative Infection Rates by U.S. State with Total Average",
          subtitle="5% - 95% HPD Intervals Colored by When State Declared State of Emergency") +
  labs(caption="As the total number of infected people is unknown, this chart measures the relative\ninfection rates between states.") +
  xlab("Days Since Outbreak Start") +
  geom_hline(yintercept = 0,linetype=3) +
  guides(fill=guide_colorbar(title="Timing of State Emergency Declaration")) +
  theme(panel.grid = element_blank(),
        legend.position = "top")
ggsave("uncertain_state_rates.png")

```

We can see from this plot that the number of infected people started to increase dramatically about March 1st. It is also clear that the infection rate significantly increased before testing occurred, as the SIR models predicted. 

It would appear that the rate of increase has leveled off in the last week, though that is not a supported inference as the scale is the logit scale, so it is similar to the log scale in that higher numbers are farther away than they appear visually. Because the latent scale is not identified, the model is showing how the infection rates have evolved from zero to the true but unknown top infection rate. As such, it appears to be slowing as it reaches the top of the scale, but that is simply an illusion of logarithmic growth. The infection rate continues to increase in the United States, which will be clearer when I apply a transformation in the next section.

As can be expected, on the whole states with earlier declarations of states of emergency have higher infection rates, which might at first lead us to believe that the declarations are associated with more, rather than fewer, infections. However, as I included a parameter for the declarations, we can estimate what independent effect the timing of the declaration had marginal of the rate of infectious growth.  To really know what the association is between states of emergency and infection rates, we can examine the relevant parameters in the model:

```{r suppress}
mcmc_hist(us_fit,regex_pars ="suppress")
```


We see that there is a statistically identifiable effect of early state of emergency declarations (`suppress_effect[1]`) on reducing the infection rate with a 5% - 95% high posterior density (HPD) interval of (-3.57,-0.23). However, there does not appear to be a clear over-time effect on the curve of the epidemic (`suppress_effect[2]`), suggesting that these early containment strategies have not yet stopped domestic transmission of the virus. In essence, what the model suggests is that early states of emergencies protected these states from later feedback effects as more neighboring states were also infected by the virus. As a result, even though these states have higher total infection rates, these rates would have been still higher if not for early state of emergency declarations.

It is important to note that this association is just that, an association. It could be that states of emergency declarations are correlated with better public health systems, and that it is in fact the public health system that is doing the work, not the timing of the state of emergency declaration. However, the fine-grained distinctions in terms of the exact day that the state of emergency was announced suggests it is indicative that those states which made early preparations are starting to see some of the results already.

As I have said, the model adjusts observed case counts for the number of test rates reported. We can see which states seem to do more tests per infected person by examining the estimated $\beta_{cq}$ parameters in this plot:

```{r tests_per_infected,fig.height=6}

test_var <- as.data.frame(us_fit,"country_test_raw") %>%
  mutate(iter=1:n()) %>%
  gather(key="variable",value="estimate",-iter) %>%
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")))

test_var <- left_join(test_var,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress))

test_var %>%
  group_by(state) %>%
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>%
  ggplot(aes(y=med_est,x=reorder(state,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est)) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  coord_flip() +
  xlab("") +
  ggtitle("Comparison of States' Testing Rates Relative to Infection Rates",
          subtitle="Based on Model of Latent COVID-19 Infection Process") +
  labs(caption = "Only relative differences between states are identified. The raw numbers do not have a
                  direct interpretation in terms of tests per infected individuals as the total number
                  of infected individuals is unknown.") +
  ylab("Proportion Tested Relative to Proportion Infected")

ggsave("testing.png",scale=1.1)

```

This plot shows that the states that are testing way more than their infected populations are actually North Dakota, Alaska and Idaho, likely because these states have yet to see very large numbers of infections. While New York has been implementing many tests, the model does not downweight infection rates considerably even with the increase in testing, implying that the increase in cases in New York is a sign of a growing infection, not just increased tests. On the more worrying side, the model estimates that Maryland, Arizona, Georgia and Delaware may be testing far fewer than are actually infected.

# Identifying the Latent Scale

To show how we can further attempt to identify the scale of the latent variable, instead of only relative differences between states, I show in this section how we can add in information from SEIR/SIR modeling to identify the total number of infected persons in the model. The crucial missing piece of information in the model is the ratio between the proportion of infected persons $I_{ct}$ and the proportion of tests per state population $q_{ct}$:

$$
\frac{q_{ct}}{I_{ct}}
$$

Another way of framing the problem is to think of how much the number of tests should increase given a one-percentage increase in the infection rate. How many of these infected people will be tested? Without an idea of the true number of infected people, it is impossible to answer this question and thus identify the latent scale.

However, an increasing number of SIR/SEIR papers show that it is likely that as few as 10% of the total number of infected persons are actually recorded as cases, including in the United States [@li2020;@perkins2020]. This number provides us with a conservative lower bound if we consider that the number of tests should be at least 10% of the total proportion of infected persons. In other words, we can consider adding the following information into the model:

$$
\frac{q_{ct}}{I_{ct}} >0.1
$$

Every percentage increase in the infection rate should result in at least a 0.1% increase in the total number of people tested as a percentage of the population. It is difficult to impose this constraint directly in the model; however, we can consider adding it as prior information if we can define a prior density over the ratio. First, for computational simplicity, we can consider a distribution over the log differences of the parameters:

$$
\text{log} q_{ct} - \text{log} I_{ct}
$$

By simulating from a uniform distribution of possible rates for $\text{log} q_{ct}$ and $\text{log} I_{ct}$, it is possible to tell that the a realistic distribution of log differences with 0.1 as a lower bound is in fact very close a standard normal distribution:

```{r std_normal}

hist(log(runif(1000,.01,.5)) - log(runif(1000,.01,.5)))

```

We can test whether it is a standard normal by comparing the log densities of different Normal distributions with different standard deviations:

```{r test_norm}

log_diff <- log(runif(1000,.01,.5)) - log(runif(1000,.01,.5))

sum(dnorm(log_diff,0,1,log=T))
sum(dnorm(log_diff,0,2,log=T))
sum(dnorm(log_diff,0,0.5,log=T))
sum(dnorm(log_diff,0,3,log=T))

```

We can see that the standard normal has the highest log density. As such, we can add the following prior to the model to bias the results to an interval between $e^-2$ and $e^2$, or  0.14 and 7.38. The prior density allows for the ratio of tests to infected individuals to be very large, suggesting that infection rates are not as high given increasing numbers of tests, but the rate is unlikely to be lower than 0.1, or 10 percent of the total number of infected. This conservative assumption allows us to use SEIR/SIR model conclusions while still permitting uncertainty over the relationship.

Because the prior is essentially a way to weight the density, I do not need to do a Jacobian adjustment as I would need to if I wanted to back-transform the density.^[For more information on this principle, see Daniel Lakeland's discussion of "masking functions" http://models.street-artists.org/2019/09/04/informative-priors-from-masking/.] For these reasons, I simply add this term to the joint posterior for each time point $t$ and country $c$:

$$
\text{log} q_{ct} - \text{log} I_{ct} \sim N(0,1)
$$

I then fit a model to the same United States data with the addition of the informative prior to scale the latent variable:


```{r run_over_id_model_scale}


if(run_model) {
  pan_model_scale <- stan_model("corona_tscs_betab_scale.stan")
  
  us_fit_scale <- sampling(pan_model_scale,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,
                           control=list(adapt_delta=0.95),
                   init=init_vals)
  
  saveRDS(us_fit_scale,"data/us_fit_scale.rds")
} else {
  us_fit_scale <- readRDS("data/us_fit_scale.rds")
}


```

I then calculate the total number of infected people in the United States by day given this re-scaled latent variable, and compare that to the observed number of cases from the New York Times data:

```{r infect_total_scaled}
all_est_state <- as.data.frame(us_fit_scale,"num_infected_high") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="variable",value="estimate",-iter) %>% 
  group_by(variable) %>% 
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")),
         time_point=as.numeric(str_extract(variable,"[1-9][0-9]?0?(?=\\])")),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state <- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by="state_num"))

# merge in total case count

case_count <- gather(cases_matrix,key="time_point",value="cases",-state) %>% 
  mutate(time_point=ymd(time_point)) %>% 
  group_by(state) %>% 
  arrange(state,time_point) %>% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count <- group_by(case_count,time_point) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state <- left_join(all_est_state,us_case_count,by="time_point")

calc_sum <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %>% 
  group_by(state_num,iter) %>% 
  arrange(state_num,time_point) %>% 
  mutate(cum_est=cumsum(estimate)) %>% 
  group_by(time_point,iter,all_cum_sum) %>% 
  summarize(us_total=sum(cum_est)) %>% 
  group_by(time_point,all_cum_sum) %>% 
  summarize(med_est=quantile(us_total,.5),
            high_est=quantile(us_total,.95),
            low_est=quantile(us_total,.05)) 

max_est <- as.integer(round(calc_sum$med_est[calc_sum$time_point==max(calc_sum$time_point)]))
high_max_est <- as.integer(round(calc_sum$high_est[calc_sum$time_point==max(calc_sum$time_point)]))
low_max_est <- as.integer(round(calc_sum$low_est[calc_sum$time_point==max(calc_sum$time_point)]))
max_obs <- calc_sum$all_cum_sum[calc_sum$time_point==max(calc_sum$time_point)]

options(scipen=999)

calc_sum %>% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est),
  fill="blue",
  alpha=0.5) +
  geom_line(aes(y=all_cum_sum)) +
  theme_minimal() +
  ylab("Total Number Infected/Reported") +
  scale_y_continuous(labels=scales::comma) +
  ggtitle("Approximate Total Number of COVID-19 Infected Individuals in the U.S.",
          subtitle="Blue 5% - 95% HPD Intervals Show Estimated Infected and Black Line Observed Cases") +
  labs(caption="These estimates are based on the assumption that as few as 10% of cases\nmay be reported based on SIR/SEIR models.") +
  annotate("text",x=ymd(c("2020-03-26","2020-03-26")),
           y=c(max_est,max_obs),
           hjust=1,
           vjust=0,
           fontface="bold",
           size=3,
           label=c(paste0("Estimated Infected:\n",formatC(low_max_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_max_est,big.mark=",",format = "f",digits=0)),
                   paste0("Total Reported Cases:\n",formatC(max_obs,big.mark=",")))) +
  xlab("Days Since Outbreak Start") +
  theme(panel.grid = element_blank(),
        legend.position = "top")

ggsave("est_vs_obs.png")

```



# Conclusion

This model was written to permit the identification of suppression measures targeted at the spread of COVID-19. It is not intended to replace the SIR/SEIR modeling literature, especially as this model relies on SIR/SEIR estimates for full identification. If anything, this modeling exercise shows why SIR/SEIR models are so important: without them it is literally impossible to know the total number of infected people on a given day. This model's simplicity and ability to use empirical data are its main features, and the hope is that it can be used and extended by researchers looking at government policies and other tertiary factors on the spread of the disease. 

To fit the model, it is necessary to have at least an estimate of how many tests have been conducted. I am currently collecting this data and am open to any additional datasets that may be available. I am also involved in a collaboration known as [CoronaNet](https://lumesserschmidt.github.io/CoronaNet/) to collect full information on government responses to the pandemic with the intention of better understanding why governments impose certain policies and what the effect of them can be. 

# Bibliography


